\documentclass{article}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage{graphicx}
\usepackage{bbm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}

\author{Siheng Zhang\\zhangsiheng@cvte.com}
\title{Chapter \textbf{\textit{3}} Generative Models}
\date{\today}      
\usepackage[a4paper,left=18mm,right=18mm,top=25mm,bottom=25mm]{geometry} 

\begin{document}
\maketitle  

This part corresponds to \textbf{Chapter 24, 31 in UML, Chapter 1, 2 in PRML}, and mainly answers the following questions:

\begin{itemize}
\item 
\item 
\end{itemize}

\tableofcontents
\newpage

\section{Naive Bayes}

	Recall that the Bayes optimal classifier (\textit{in Chapter 1, Ex6}) is:
	
	\begin{equation*}
	h_{\mathrm{Bayes}}(\bm{x}) = \arg\max\limits_{y\in\{0,1\}} p (Y=y|X=\bm{x})
	\end{equation*}
	
	To describe the posterior probability function we need $2^d$ parameters, this implies that the number of examples we need grows exponentially with the number of features. To avoid this problem, we assume that given the label, the features are independent of each other, i.e., 
	
	\begin{equation*}
	p (X=\bm{x}|Y=y) = \prod_{i=1}^d p (X_i=x_i|Y=y)
	\end{equation*}
	
	Together with Bayes' rule, the Bayes optimal classifier can be simplified as:
	
	\begin{equation}
	h_{\mathrm{Bayes}}(\bm{x}) = \arg\max\limits_{y\in\{0,1\}} p (Y=y) \prod_{i=1}^d p (X_i=x_i|Y=y)
	\end{equation}
Now the number of parameters we need to estimate is only $2d + 1$. When we also estimate the parameters using the maximum likelihood principle (see below), the resulting classifier is called the \textit{Naive Bayes} classifier.

\section{Density estimation}

	To apply the Bayesian decision principle, we should know the probability distribution. In fact, machine learning can be treated as '\textbf{\textit{fitting the underlying distribution}}' (see \ref{sec:final}). There are two classes of methods for estimation, parametric and non-parametric methods.
	
	\subsection{Parametric method: maximum likelihood}
	Assume that the form of distribution is known, the problem is to estimate the parameters. Specifically, given an i.i.d. training set $S = (\bm{x}_1,\cdots,\bm{x}_m)$ sampled according to a density distribution, the likelihood of $S$ given $\theta$ is:
	\begin{equation*}
	L(S;\theta) = \prod_{i=1}^m  p(\bm{x}_i;\theta)
	\end{equation*}
Usually, we turn to optimize its logarithm, that is
	\begin{equation}
	\log L(S;\theta) = \sum_{i=1}^m \log p(\bm{x}_i;\theta)
	\end{equation}
	\begin{itemize}
	\item [\textbf{1}] Bernoulli distribution, $\theta={\mu}$
	Bernoulli distribution describes the probability of a binary variable (here we consider only the scalar case) $x$. The probability of $x=1$ is denoted by parameter $\mu$, and of $x=0$ is $1-\mu$, so,
	\begin{equation*}
	p(x;\theta)=\mu^x(1-\mu)^{(1-x)}
	\end{equation*}
	The log likelihood function is given by
	\begin{equation*}
	\log L(S;\theta) = \sum_{i=1}^m \log p(x_i;\theta) = \sum_{i=1}^m x_i\log \mu + (1-x_i)\log(1-\mu)
	\end{equation*}
The derivative of the log likelihood with respect to $\mu$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \bm{\mu}} = \sum_{i=1}^m \frac{x_i}{\mu} + \frac{1-x_i}{1-\mu}
	\end{equation*}
	\begin{footnotesize}
	\textit{\underline{remark1}}: 
	\end{footnotesize}
	
	\item [\textbf{2}] Multinomial distribution
	
	\begin{footnotesize}
	\textit{\underline{remark2}}:
	\end{footnotesize}
	
	\item [\textbf{3}] Gaussian distribution, $\theta=(\bm{\mu},\bm{\Sigma})$
	
	The log likelihood function is given by
	\begin{equation*}
	\begin{split}
	\log L(S;\theta) = \sum_{i=1}^m \log p(\bm{x}_i;\theta) &= \sum_{i=1}^m \log \left[ \frac{1}{(2\pi)^{d/2} |\bm{\Sigma}|^{1/2}} \exp \left( -\frac{1}{2}(\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu}) \right) \right]  \\
	&= \sum_{i=1}^m \left[\frac{-d}{2} \log (2\pi) - \frac{1}{2}\log |\bm{\Sigma}| -\frac{1}{2}(\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu}) \right]  \\
	&= \frac{-md}{2} \log (2\pi) - \frac{m}{2}\log |\bm{\Sigma}| - \frac{1}{2} \sum_{i=1}^m (\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})
	\end{split}
	\end{equation*}
	The derivative of the log likelihood with respect to $\bm{\mu}$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \bm{\mu}} = \sum_{i=1}^m \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})
	\end{equation*}
and setting it to zero leads to:
	
	\begin{equation}
	\bm{\mu}_{\mathrm{ML}}= \frac{\sum_{i=1}^m \bm{x}_i}{m}
	\end{equation}

	\begin{footnotesize}
	\textit{\underline{remark3}}: Deriving $\bm{\Sigma}$ requires the use of the following linear algebra and calculus properties:	
	\begin{itemize}
	\item The trace is invariant under cyclic permutation of matrix products: $tr[\bm{A}\bm{B}\bm{C}]=tr[\bm{C}\bm{A}\bm{B}]=tr[\bm{B}\bm{C}\bm{A}]$;
	\item Since $\bm{x}^\top \bm{A} \bm{x}$ is a scalar, its trace is itself, and hence $\bm{x}^\top \bm{A} \bm{x} = tr[\bm{x}^\top \bm{A} \bm{x}] = tr[\bm{x} \bm{x}^\top \bm{A}]$;
	\item $\frac{\partial tr[\bm{A} \bm{B}]}{\partial \bm{A}} = \bm{B}^\top$;
	\item $\frac{\partial \log |\bm{A}|}{\partial \bm{A}} = (\bm{A}^{-1})^\top$;
	\item $\frac{\partial tr(\bm{A}\bm{X}^{-1}\bm{B})}{\partial \bm{X}} = -(\bm{X}^{-1} \bm{BA}\bm{X}^{-1})^\top$
	\end{itemize}
	\end{footnotesize}
	The derivative of the log likelihood with respect to $\bm{\Sigma}$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \bm{\Sigma}} = -\frac{m}{2} (\bm{\Sigma}^{-1})^\top + \frac{1}{2} \sum_{i=1}^m  \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})(\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1}
	\end{equation*}
Here we does not give a formal proof that $\bm{\Sigma}$ is symmetric but directly using this conclusion, and setting the derivative to zero leads to:
	\begin{equation}
	\bm{\Sigma}_{\mathrm{ML}} = \sum_{i=1}^m \frac{(\bm{x}_i - \bm{\mu}_{\mathrm{ML}})(\bm{x}_i - \bm{\mu}_{\mathrm{ML}})^\top}{m}
	\end{equation}
	
	\begin{footnotesize}
	\textit{\underline{remark4}}: Again, we consider the property of MLE result. The estimation of $\bm{\mu}$ is unbiased,
	\begin{equation*}
	\mathbb{E} (\bm{\mu}_\mathrm{ML}) = \mathbb{E} \left(\frac{\sum_{i=1}^m \bm{x}_i}{m}\right) =  \sum_{i=1}^m  \frac{\mathbb{E}(\bm{x}_i)}{m} = \mathbb{E}(\bm{x}) = \bm{\mu}
	\end{equation*}
However, the estimation of $\bm{\Sigma}$ is biased,
	\begin{equation*}
	\mathbb{E} (\bm{\Sigma}_\mathrm{ML}) = \sum_{i=1}^m  \frac{\mathbb{E}((\bm{x}_i - \bm{\mu}_\mathrm{ML})(\bm{x}_i - \bm{\mu}_\mathrm{ML})^\top)}{m}
	= \sum_{i=1}^m  \frac{\mathbb{E}(\bm{x}_i \bm{x}_i^\top) + \mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{\mu}_\mathrm{ML}^\top) - 2\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{x}_i^\top)}{m}
	\end{equation*}
	Consider each term in the numerator, note that each pair of samples is independent,
	\begin{equation*}
	\begin{split}
	\mathbb{E}(\bm{x}_i \bm{x}_i^\top) &= \bm{\Sigma} + \bm{\mu}\bm{\mu}^\top  \\
	\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{\mu}_\mathrm{ML}^\top) &= 
	\frac{1}{m^2} \mathbb{E}\left(\sum_{i=1}^m \sum_{j=1}^m \bm{x}_i \bm{x}_j^\top\right) 
	= \frac{1}{m^2} \mathbb{E}\left(\sum_{i=1}^m \sum_{j=1}^m (\bm{x}_i-\bm{\mu}) (\bm{x}_j-\bm{\mu})^\top + 2\bm{\mu} \sum_{i=1}^m (\bm{x}_i-\bm{\mu})^\top + \sum_{i=1}^m\sum_{j=1}^m \bm{\mu}\bm{\mu}^\top \right) \\
	&= \frac{1}{m^2} \left( \sum_{i=1}^m \mathbb{E}((\bm{x}_i-\bm{\mu}) (\bm{x}_i-\bm{\mu})^\top)+ m^2 \bm{\mu}\bm{\mu}^\top \right)
	= \frac{\bm{\Sigma}}{m} + \bm{\mu}\bm{\mu}^\top \\
	\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{x}_i^\top) &=  \mathbb{E} \left(\frac{1}{m}\sum_{j=1}^m \bm{x}_j \bm{x}_i^\top \right)
	= \frac{1}{m} \mathbb{E} \left(\sum_{j=1}^m (\bm{x}_j-\bm{\mu})(\bm{x}_i-\bm{\mu})^\top + \bm{\mu}\sum_{j=1}^m (\bm{x}_j-\bm{\mu})^\top + \sum_{j=1}^m \bm{\mu} (\bm{x}_i-\bm{\mu})^\top  + \sum_{j=1}^m \bm{\mu} \bm{\mu}^\top\right) \\
	&= \frac{\bm{\Sigma}}{m} + \bm{\mu} \bm{\mu}^\top
	\end{split}
	\end{equation*}
Hence, $\mathbb{E} (\bm{\Sigma}_\mathrm{ML}) = \frac{m-1}{m} \bm{\Sigma}$ which is biased.
	\end{footnotesize}
	
	\item [\textbf{4}] Periodic variables
	\item [\textbf{5}] Exponential family
	\end{itemize}
	
	\subsection{Expectation maximization: maximum likelihood for partial observed data}
	Until now, a training sequence is $\{(\bm{x}_1,y_1),\cdots,(\bm{x}_m,y_m)\}$, in which $y_i$ is the latent factor that depends whether $\rm{x}_i$ is sampled from. However, if the latent factors are not observed, the likelihood of the sequence $\{\bm{x}_1,\cdots,\bm{x}_m\}$ is:
	\begin{equation*}
	L(S;\theta) = \prod_{i=1}^m \sum_{j=1}^k p_\theta(x_i,y_i) = \prod_{i=1}^m \sum_{j=1}^k p_\theta(x_i|y_i)p_\theta(y_i)
	\end{equation*}
	The maximum-likelihood estimator is therefore the solution of the maximization problem:
	\begin{equation}
	\log L(S;\theta) = \sum{i=1}^m \log p_\theta(x_i|y_i)p_\theta(y_i)
	\end{equation}
	\subsection{Non-parametric methods}

\section{Bayesian Reasoning}

\section{Generative models}
	\subsection{GMM (Gaussian mixture models)}
	\subsection{HMM (Hidden Markov models)}

\section{\textit{v.s.} discriminant models}
	\label{sec:final}
	In generative approaches, it is assumed that the underlying distribution over the data has a specific parametric form and the goal is to estimate the parameters of the model. But in discriminative approaches, the goal is rather to learn an accurate predictor directly. 
	
	Of course, if we succeed in learning the underlying distribution accurately, prediction from the Bayes optimal classifier is reliable. The problem is that, it is usually more difficult to learn the underlying distribution than to learn an accurate predictor. This was phrased by Vladimir Vapnik:
	\begin{center}
	\textit{"When solving a given problem, try to avoid a more general problem as an intermediate step."}
	\end{center}

	However, in some situations, it is reasonable to adopt the generative models. Sometimes it is easier (computationally) to estimate the parameters of the model than to learn a discriminative predictor. Additionally, in some cases we do not have a specific task at hand but rather would like to use the data at a later time.
	
	Modern generative models have another big goal, that is to 'generate' (sample from the underlying distribution) data like that in the real world. The intuition behind this approach follows a famous quote from Richard Feynman:	
	\begin{center}
	\textit{"What I cannot create, I do not understand."}
	\end{center}

	\subsection{Naive Bayes to linear discriminant models}
	The usual assumption in Naive Bayes classifier is that each conditional probability $p(X=\bm{x}|Y=y)$ is a Gaussian distribution. Consider the binary classification task, denote the two conditional distribution as $\mathcal{N}(\bm{\mu}_0,\bm{\Sigma}_0)$, $\mathcal{N}(\bm{\mu}_1,\bm{\Sigma}_1)$, we will predict $h_{\mathrm{Bayes}}(\bm{x})=1$ iff.
	
	\begin{equation*}
	\begin{split}
	&\frac{p(Y=0) p(X=\bm{x}|Y=0)}{p(Y=1) p(X=\bm{x}|Y=1)} > 1 \\
	&\iff \log \frac{p(Y=0)}{p(Y=1)} + \log p(X=\bm{x}|Y=0) - \log p(X=\bm{x}|Y=1) > 0 \\
	&\iff -\frac{1}{2}(\bm{x}-\bm{\mu}_0)^\top \bm{\Sigma}_0^{-1}(\bm{x}-\bm{\mu}_0) + \frac{1}{2}(\bm{x}-\bm{\mu}_1)^\top \bm{\Sigma}_1^{-1}(\bm{x}-\bm{\mu}_1) +  \frac{1}{2}\log\frac{|\bm{\Sigma}_1|}{|\bm{\Sigma}_0|} + \log \frac{p(Y=0)}{p(Y=1)} > 0 \\
	&\iff \frac{1}{2} \bm{x}^\top ( \bm{\Sigma}_1^{-1} - \bm{\Sigma}_0^{-1}) \bm{x} + (\bm{\mu}_0^\top\bm{\Sigma}_0^{-1} - \bm{\mu}_1^\top\bm{\Sigma}_1^{-1}) \bm{x} + \underbrace{\frac{1}{2} (\bm{\mu}_1^\top\bm{\Sigma}_1^{-1}\bm{\mu}_1 - \bm{\mu}_0^\top\bm{\Sigma}_0^{-1}\bm{\mu}_0 ) + \frac{1}{2}\log\frac{|\bm{\Sigma}_1|}{|\bm{\Sigma}_0|} + \log \frac{p(Y=0)}{p(Y=1)}}_{\mathrm{b}} > 0
	\end{split}
	\end{equation*}
which is a quadratic discriminant function.

	Further, if we assume that $\bm{\Sigma}_0=\bm{\Sigma}_1=\bm{\Sigma}$, the classifier can be simplified to be a linear discriminant function $\bm{w}\cdot\bm{x}+b$, with $\bm{w}=(\bm{\mu}_0 - \bm{\mu}_1)^\top\bm{\Sigma}^{-1}$ and $b=\frac{1}{2} (\bm{\mu}_1^\top\bm{\Sigma}^{-1}\bm{\mu}_1 - \bm{\mu}_0^\top\bm{\Sigma}^{-1}\bm{\mu}_0 ) + \log \frac{p(Y=0)}{p(Y=1)}$. If the prior probability is equal, namely $p(Y=0)=p(Y=1)$, the bias term can be further simplified.
	
	
	
	
\section{Exercises and solutions}

\textit{
	  Chapter 4. Decision stumps, ensemble learning, Bayes PAC \\
      Chapter 5. Linear models, perceptron, MLP, deep learning, Generalization bounds on deep learning.}

\end{document}