\documentclass{article}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=cyan,      
	urlcolor=red,
	citecolor=green,
}
\usepackage{bbm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}
\setlength{\parindent}{2em}
\author{Siheng Zhang\\zhangsiheng@cvte.com}
\title{Chapter \textbf{\textit{3}} Generative Models}
\date{\today}      
\usepackage[a4paper,left=18mm,right=18mm,top=25mm,bottom=25mm]{geometry} 

\begin{document}
\maketitle  

This part corresponds to \textbf{Chapter 24, 31 in UML, Chapter 1, 2 in PRML}, and mainly answers the following questions:

\begin{itemize}
\item How to bring Bayes Optimal classifier into application? (Feature independent assumption)
\item To estimate the class conditional probability distribution for Bayes classifier, we study both the parametric (\textit{includes a family of basic probability distributions}) and non-parametric methods.
\item A glance for generative and discriminant models. Naive Bayes, GMM, and etc, belong to the former, which requires estimation of underlying distribution. This is more general and hence difficult. Discriminant models try to avoid it by optimization.
\item Last but not the least, there is a connection between generative and discriminant models. At last of this chapter, we point out how to derive a linear discriminant from Bayes classifier. As we will see in the next chapter, discriminant with penalization also has a intrinsic connection with generative models with some prior distribution.
\end{itemize}

\tableofcontents
\newpage

\section{Naive Bayes}

	Recall that the Bayes optimal classifier (\textit{in Chapter 1, Ex6}) is:
	
	\begin{equation*}
	h_{\mathrm{Bayes}}(\bm{x}) = \arg\max\limits_{y\in\{0,1\}} p (Y=y|X=\bm{x})
	\end{equation*}
	
	To describe the posterior probability function we need $2^d$ parameters, this implies that the number of examples we need grows exponentially with the number of features. To avoid this problem, we assume that given the label, the features are independent of each other, i.e., 
	
	\begin{equation*}
	p (X=\bm{x}|Y=y) = \prod_{i=1}^d p (X_i=x_i|Y=y)
	\end{equation*}
	
	Together with Bayes' rule, the Bayes optimal classifier can be simplified as:
	
	\begin{equation}
	h_{\mathrm{Bayes}}(\bm{x}) = \arg\max\limits_{y\in\{0,1\}} p (Y=y) \prod_{i=1}^d p (X_i=x_i|Y=y)
	\end{equation}
Now the number of parameters we need to estimate is only $2d + 1$. When we also estimate the parameters using the maximum likelihood principle (see below), the resulting classifier is called the \textit{Naive Bayes} classifier.

\section{Density estimation}

	To apply the Bayesian decision principle, we should know the probability distribution. In fact, machine learning can be treated as '\textbf{\textit{fitting the underlying distribution}}' (see \ref{sec:final}). There are two classes of methods for estimation, parametric and non-parametric methods.
	
	\subsection{Parametric method: maximum likelihood}
	Assume that the form of distribution is known, the problem is to estimate the parameters. Specifically, given an i.i.d. training set $S = (\bm{x}_1,\cdots,\bm{x}_m)$ sampled according to a density distribution, the likelihood of $S$ given $\theta$ is:
	\begin{equation*}
	L(S;\theta) = \prod_{i=1}^m  p(\bm{x}_i;\theta)
	\end{equation*}
Usually, we turn to optimize its logarithm, that is
	\begin{equation}
	\log L(S;\theta) = \sum_{i=1}^m \log p(\bm{x}_i;\theta)
	\end{equation}
	
	Following is the examples:
	
	\begin{itemize}
	\item [\textbf{1}] Bernoulli distribution, $\theta={\mu}$
	
	Bernoulli distribution describes the probability of a binary variable $x$. The probability of $x=1$ is denoted by parameter $\mu$, and of $x=0$ is $1-\mu$, so, $p(x;\theta)=\mu^x(1-\mu)^{(1-x)}$. The log likelihood function is given by
	\begin{equation*}
	\log L(S;\theta) = \sum_{i=1}^m \log p(x_i;\theta) = \sum_{i=1}^m x_i\log \mu + (1-x_i)\log(1-\mu)
	\end{equation*}
The derivative of the log likelihood with respect to $\mu$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \mu} = \sum_{i=1}^m \frac{x_i}{\mu} - \frac{1-x_i}{1-\mu} = \sum_{i=1}^m \frac{x_i-\mu}{\mu(1-\mu)}
	\end{equation*}
which leads to $\mu_{\mathrm{ML}}=\frac{1}{m}\sum_{i=1}^m x_i$.
	\begin{footnotesize}
		
	
	
	\textit{\underline{remark1}}: $\theta_{\mathrm{ML}}$, in intrinsic, is a function of observed random variables, and hence we can calculate its expectation. If the expectation of an estimation is exactly the parameter in theory, we say that the estimation is unbiased. In this example,
	\begin{equation*} 
	\mathbb{E} (\mu_\mathrm{ML}) = \mathbb{E} \left(\frac{\sum_{i=1}^m x_i}{m}\right) =  \sum_{i=1}^m  \frac{\mathbb{E}(x_i)}{m} = \mathbb{E}(x) = \mu
	\end{equation*}
	\end{footnotesize}
		
	
	
	\item [\textbf{2}] Multinomial distribution, $\theta=\bm{\mu}$
	
	Multinomial distribution extends the binary variable to one of $d$ possible value. The random variable can be represented by a $d$-dimensional vector $\bm{x}$, in which only one element equals 1 and others equal 0. Denote the probability of $x_j=1$ by $\mu_j$, then the distribution is given by
	
	\begin{equation*}
	p(\bm{x}|\bm{\mu}) = \prod_{j=1}^d \mu_j^{x_j}
	\end{equation*}
in which $\sum_{j=1}^d \mu_j=1, \mu_j\geq 0$.
		
	The corresponding log likelihood function is given by
	\begin{equation*}
	\log L(S;\theta) = \sum_{i=1}^m \log p(\bm{x}_i;\theta) = \sum_{i=1}^m \sum_{j=1}^d x_{ij} \log \mu_j
	\end{equation*}
To maximize it with respect to $\mu_j$ must take account of the constraint that $\sum_{j=1}^d \mu_j=1$. Using Lagrange multiplier $\lambda$, it is equivalent to maximize 
	
	\begin{equation*}
	L' = \sum_{i=1}^m \sum_{j=1}^d x_{ij} \log \mu_j + \lambda \left( \sum_{j=1}^d \mu_j - 1 \right)
	\end{equation*}
Take derivative with regard to  $\mu_j$
	\begin{equation*}
	\frac{\partial L'}{\partial \mu_j} = \sum_{i=1}^m\frac{x_{ij}}{\mu_j} + \lambda
	\end{equation*}
which leads to $\mu_{j,\mathrm{ML}} = -\sum_{i=1}^m x_{ij}/\lambda$. Besides, $\sum_{j=1}^d \mu_j=-\sum_{j=1}^d \sum_{i=1}^m  x_{ij}/\lambda = -m/\lambda=1$, thereby leading to $\lambda=-m$. Hence, $\bm{\mu}_{\mathrm{ML}} = \frac{1}{m}\sum_{i=1}^m \bm{x}_i$, which is also unbiased.
	
	\item [\textbf{3}] Gaussian distribution, $\theta=(\bm{\mu},\bm{\Sigma})$
	The Gaussian distribution is 
	\begin{equation}
	p(\bm{x}) = \frac{1}{(2\pi)^{d/2} |\bm{\Sigma}|^{1/2}} \exp \left( -\frac{1}{2} (\bm{x} - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu})\right)
	\end{equation}
	The log likelihood function is given by
	\begin{equation*}
	\log L(S;\theta) = \sum_{i=1}^m \log p(\bm{x}_i;\theta) 
	= \frac{-md}{2} \log (2\pi) - \frac{m}{2}\log |\bm{\Sigma}| - \frac{1}{2} \sum_{i=1}^m (\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})
	\end{equation*}
	The derivative of the log likelihood with respect to $\bm{\mu}$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \bm{\mu}} = \sum_{i=1}^m \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})
	\end{equation*}
and setting it to zero leads to: $\bm{\mu}_{\mathrm{ML}}= \sum_{i=1}^m \bm{x}_i / m$.
	
	
	
	\begin{footnotesize}
	\textit{\underline{remark2}}: Deriving $\bm{\Sigma}$ requires the use of the following linear algebra and calculus properties:	
	\begin{itemize}
	\item The trace is invariant under cyclic permutation of matrix products: $tr[\bm{A}\bm{B}\bm{C}]=tr[\bm{C}\bm{A}\bm{B}]=tr[\bm{B}\bm{C}\bm{A}]$;
	\item Since $\bm{x}^\top \bm{A} \bm{x}$ is a scalar, its trace is itself, and hence $\bm{x}^\top \bm{A} \bm{x} = tr[\bm{x}^\top \bm{A} \bm{x}] = tr[\bm{x} \bm{x}^\top \bm{A}]$;
	\item $\frac{\partial tr[\bm{A} \bm{B}]}{\partial \bm{A}} = \bm{B}^\top$;
	\item $\frac{\partial \log |\bm{A}|}{\partial \bm{A}} = (\bm{A}^{-1})^\top$;
	\item $\frac{\partial tr(\bm{A}\bm{X}^{-1}\bm{B})}{\partial \bm{X}} = -(\bm{X}^{-1} \bm{BA}\bm{X}^{-1})^\top$
	\end{itemize}
	\end{footnotesize}
	
	
	
	The derivative of the log likelihood with respect to $\bm{\Sigma}$ is given by
	\begin{equation*}
	\frac{\partial \log L(S;\theta)}{\partial \bm{\Sigma}} = -\frac{m}{2} (\bm{\Sigma}^{-1})^\top + \frac{1}{2} \sum_{i=1}^m  \bm{\Sigma}^{-1} (\bm{x}_i - \bm{\mu})(\bm{x}_i - \bm{\mu})^\top \bm{\Sigma}^{-1}
	\end{equation*}
Here we does not give a formal proof that $\bm{\Sigma}$ is symmetric but directly using this conclusion, and setting the derivative to zero leads to $\bm{\Sigma}_{\mathrm{ML}} = \sum_{i=1}^m (\bm{x}_i - \bm{\mu}_{\mathrm{ML}})(\bm{x}_i - \bm{\mu}_{\mathrm{ML}})^\top/m $
		
	\begin{footnotesize}
	\textit{\underline{remark3}}: Again, the estimation of $\bm{\mu}$ is unbiased. However, the estimation of $\bm{\Sigma}$ is biased,
	\begin{equation*}
	\mathbb{E} (\bm{\Sigma}_\mathrm{ML}) = \sum_{i=1}^m  \frac{\mathbb{E}((\bm{x}_i - \bm{\mu}_\mathrm{ML})(\bm{x}_i - \bm{\mu}_\mathrm{ML})^\top)}{m}
	= \sum_{i=1}^m  \frac{\mathbb{E}(\bm{x}_i \bm{x}_i^\top) + \mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{\mu}_\mathrm{ML}^\top) - 2\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{x}_i^\top)}{m}
	\end{equation*}
	Consider each term in the numerator, note that each pair of samples is independent,
	\begin{equation*}
	\begin{split}
	\mathbb{E}(\bm{x}_i \bm{x}_i^\top) &= \bm{\Sigma} + \bm{\mu}\bm{\mu}^\top  \\
	\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{\mu}_\mathrm{ML}^\top) &= 
	\frac{1}{m^2} \mathbb{E}\left(\sum_{i=1}^m \sum_{j=1}^m \bm{x}_i \bm{x}_j^\top\right) 
	= \frac{1}{m^2} \mathbb{E}\left(\sum_{i=1}^m \sum_{j=1}^m (\bm{x}_i-\bm{\mu}) (\bm{x}_j-\bm{\mu})^\top + 2\bm{\mu} \sum_{i=1}^m (\bm{x}_i-\bm{\mu})^\top + \sum_{i=1}^m\sum_{j=1}^m \bm{\mu}\bm{\mu}^\top \right) \\
	&= \frac{1}{m^2} \left( \sum_{i=1}^m \mathbb{E}((\bm{x}_i-\bm{\mu}) (\bm{x}_i-\bm{\mu})^\top)+ m^2 \bm{\mu}\bm{\mu}^\top \right)
	= \frac{\bm{\Sigma}}{m} + \bm{\mu}\bm{\mu}^\top \\
	\mathbb{E}(\bm{\mu}_\mathrm{ML} \bm{x}_i^\top) &=  \mathbb{E} \left(\frac{1}{m}\sum_{j=1}^m \bm{x}_j \bm{x}_i^\top \right)
	= \frac{1}{m} \mathbb{E} \left(\sum_{j=1}^m (\bm{x}_j-\bm{\mu})(\bm{x}_i-\bm{\mu})^\top + 2\bm{\mu}\sum_{j=1}^m (\bm{x}_j-\bm{\mu})^\top + \sum_{j=1}^m \bm{\mu} \bm{\mu}^\top\right) = \frac{\bm{\Sigma}}{m} + \bm{\mu} \bm{\mu}^\top
	\end{split}
	\end{equation*}
Hence, $\mathbb{E} (\bm{\Sigma}_\mathrm{ML}) = \frac{m-1}{m} \bm{\Sigma}$ which is biased.
	\end{footnotesize}
	
	
	
	\item [\textbf{4}] Exponential family
	The exponential family is defined to be the set of distributions of the form
	\begin{equation}
	p(\bm{x}|\bm{\eta}) = h(\bm{x}) \exp\{ \bm{\eta}^\top \bm{u}(\bm{x}) - A(\bm{\eta}) \}
	\end{equation}
	
	
	
	\begin{footnotesize}
	\textit{\underline{remark4}}: Bernoulli distribution is a member in this family,
	\begin{equation*}
	p(x|\mu) = \mu^x(1-\mu)^{1-x} = \exp \{ x \log \mu + (1-x) \log (1-\mu) \} = \exp\left\{ \log \left( \frac{\mu}{1-\mu}\right) x + \log(1-\mu)\right\}
	\end{equation*}
	Compare with the general form shows that $h(x)=1,u(x)=x, \eta=\log \frac{\mu}{1-\mu} $, and $A(\eta)=\log (1+\exp(\eta))$.

	\textit{\underline{remark5}}: Multinomial distribution is a member in this family. Recall that multinomial distribution indeed has $d-1$ parameters since $\sum_{j=1}^d \mu_d = 1$, we have
	\begin{equation*}
	\begin{split}
	p(\bm{x}|\bm{\mu}) &= \prod_{j=1}^d \mu_j^{x_j} = \exp\left\{ \sum_{j=1}^d x_j \log \mu_j \right\} = \exp\left\{ \sum_{j=1}^{d-1} x_j \log \mu_j + \left(1-\sum_{j=1}^{d-1} x_j \right) \log \left(1-\sum_{j=1}^{d-1} \mu_j \right) \right\} \\
	&= \exp\left\{ \sum_{j=1}^{d-1} x_j \log \left( \frac{\mu_j}{1-\sum_{k=1}^{d-1} \mu_k} \right) +  \log \left(1-\sum_{j=1}^{d-1} \mu_j \right) \right\}
	\end{split}
	\end{equation*}
	Define $\eta_j =  \log \frac{\mu_j}{1-\sum_{k=1}^d \mu_k}$, then $\mu_j = \frac{\exp\eta_j}{1+\sum_{k=1}^d \exp \eta_k}$, and $1-\sum_{j=1}^{d-1} \mu_j = 1-\frac{\sum_{j=1}^{d-1} \exp \eta_j}{1+\sum_{k=1}^d \exp \eta_k}=\frac{\exp \eta_{d}}{1+\sum_{k=1}^d \exp \eta_k}$. Compare with the general form shows that $h(\bm{x})=1, u(\bm{x})=\bm{x}, A(\bm{\eta})=\log (1+\sum_{k=1}^d \exp \eta_k)-\eta_d$.
	
	\textit{\underline{remark6}}: Gaussian distribution is a member in this family.
	\begin{equation*}
	p(\bm{x}|\bm{\mu}) = \frac{1}{(2\pi)^{d/2} |\bm{\Sigma}|^{1/2}} \exp \left( -\frac{1}{2} \bm{x}^\top \bm{\Sigma}^{-1} \bm{x}  -\frac{1}{2} \bm{\mu}^\top \bm{\Sigma}^{-1} \bm{\mu} + \bm{\mu}^\top \bm{\Sigma}^{-1} \bm{x} \right)
	\end{equation*}
	Since $\bm{x}^\top \bm{\Sigma}^{-1} \bm{x} = tr [\bm{x}^\top \bm{\Sigma}^{-1} \bm{x}] = tr [\bm{\Sigma}^{-1} \bm{x} \bm{x}^\top ]$. Compare with the general form shows that $h(\bm{x})=(2\pi)^{-d/2}, u(\bm{x})=(1, \bm{x}, \bm{x}\bm{x}^\top)^\top, \bm{\eta}=(-\frac{1}{2} \bm{\mu}^\top \bm{\Sigma}^{-1} \bm{\mu}-\frac{1}{2}\log|\bm{\Sigma}|, \bm{\Sigma}^{-1}\bm{\mu}, -\frac{1}{2} \bm{\Sigma}^{-1})^\top$.
	\end{footnotesize}
	
	Now consider the problem of estimating the parameter vector $\bm{\mu}$ in the general exponential family distribution. The log likelihood function is given by
	
	\begin{equation*}
	\sum_{i=1}^m \log h(\bm{x}_i) + \bm{\eta}^\top \sum_{i=1}^m u(\bm{x}_i) - \sum_{i=1}^m A(\bm{\eta})
	\end{equation*}
Take derivative with regard to $\bm{\eta}$ leas to $\frac{\partial A(\bm{\eta})}{\partial \bm{\eta}} = \sum_{i=1}^m \bm{x}_i/m$, which can in principle be solved to obtain $\bm{\eta}_\mathrm{ML}$.
	
	Note that $\int  h(\bm{x}) \exp\{ \bm{\eta}^\top \bm{u}(\bm{x}) - A(\bm{\eta}) \} = 1$. Take derivatives of both sides with regard to $\bm{\eta}$, we have,
	\begin{equation*}
	\int  h(\bm{x}) \exp\{ \bm{\eta}^\top \bm{u}(\bm{x}) - A(\bm{\eta}) \} \left(\bm{u}(\bm{x}) - \frac{\partial A(\bm{\eta})}{\partial \eta} \right) = 0
	\end{equation*}
which leads to
	\begin{equation}
	\frac{\partial A(\bm{\eta})}{\partial \bm{\eta}} = \mathbb{E} [u(\bm{x})]
	\end{equation}

	Therefore, $\sum_i u(\bm{x}_i)$ is called the sufficient statistic. Also note that the covariance of $u(\bm{x})$ can be expressed in terms of the second derivatives $A(\bm{\eta})$, and similarly for higher order moments. Thus, provided we can normalize a distribution from the exponential family, we can always find its moments by simple differentiation.
	\end{itemize}
	
	\subsection{Non-parametric methods}

\section{Bayesian Reasoning}

	Maximum likelihood estimation can give severely over-fitted results for small data sets. To address this problem, we develop a Bayesian treatment, which introduce a prior distribution $p(\bm{\mu})$. To determine the prior distribution, we expect that the posterior distribution will have the same functional form as the prior. This is called \textbf{conjugacy}, and the prior is called \textbf{conjugate prior}.
	
	\begin{itemize}
	\item [\textbf{1}] Beta distribution for Bernoulli distribution
	
	Recall that the likelihood of Bernoulli distribution is proportional to $\mu^x (1-\mu)^{1-x}$,	we choose a prior to be proportional to powers of $\mu$ and $1-\mu$, then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior.
	
	The Beta distribution $\mu\sim \mathrm{Beta}(a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$, in which $\Gamma(x)=\int^\infty_0 t^{x-1} e^{-t} \mathrm{d} t$, meets the requirement. Indeed, given the observed sequence $S$, 
	\begin{equation*}
	\begin{split}
	p(\mu|S) &= \frac{p(S|\mu)}{p(S)}=\frac{p(S|\mu)}{\int p(S|\mu)p(\mu) \mathrm{d} \mu}
	= \frac{\prod_{i=1}^m p(x_i;\mu)}{\int \prod_{i=1}^m p(x_i;\mu)\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} \mathrm{d} \mu} \\
	&= \frac{\prod_{i=1}^m \mu^{x_i} (1-\mu)^{1-x_i}}{\int \prod_{i=1}^m p(x_i;\mu)\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} \mathrm{d} \mu} \\
	&= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \frac{\mu^{\sum_{i=1}^m x_i} (1-\mu)^{m-\sum_{i=1}^m x_i}}{\int \prod_{i=1}^m \mu^{\sum_{i=1}^m+a-1} (1-\mu)^{m-\sum_{i=1}^m x_i +b-1} \mathrm{d} \mu} \\
	&= \mathrm{Beta}(a+\sum_{i=1}^m x_i, b+m-\sum_{i=1}^m x_i)
	\end{split}
	\end{equation*}
	
	\item [\textbf{2}] Dirichlet distribution for multinomial distribution
	\item [\textbf{3}] Gaussian distribution
	\item [\textbf{4}] Exponential distribution
	\end{itemize}
	
\section{EM algorithm: MLE for partial observed data}
	Until now, a training sequence is $\{(\bm{x}_1,y_1),\cdots,(\bm{x}_m,y_m)\}$, in which $y_i$ is the latent factor that depends whether $\rm{x}_i$ is sampled from. However, if the latent factors are not observed, the likelihood of the sequence $\{\bm{x}_1,\cdots,\bm{x}_m\}$ is:
	\begin{equation*}
	L(S;\theta) = \prod_{i=1}^m \sum_{j=1}^k p_\theta(\bm{x}_i,y_j) = \prod_{i=1}^m \sum_{j=1}^k p_\theta(\bm{x}_i|y_j)p_\theta(y_j)
	\end{equation*}
	The maximum-likelihood estimator is therefore the solution of the maximization problem:
	\begin{equation}
	\log L(S;\theta) = \sum_{i=1}^m \log \sum_{j=1}^k p_\theta(\bm{x}_i|y_j)p_\theta(y_j)
	\end{equation}
	
	In the E-step, we use the current parameter values $\theta^{\mathrm{old}}$ to find the posterior distribution of the latent variables given by $p(\bm{Y}|\bm{X}, \theta^{\mathrm{old}})$. We then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value $\theta$. This expectation, denoted , is given by
	
	\subsection{EM for GMM}
	
	GMM (Gaussian mixture models) is a typical example, with parameters comprising the means and covariances of the components and the mixing coefficients. Its log-likelihood function (plus a Lagrange multiplier) is given by
	\begin{equation*}
	\sum_{i=1}^m \log \sum_{j=1}^k \pi_j \mathcal{N} (\bm{x}_i|\bm{\mu}_j,\bm{\Sigma}_j) + \lambda \left(\sum_{j=1}^k \pi_j - 1\right)
	\end{equation*}
	Take derivatives with regard to $\bm{\mu}_k$, we have
	
	\begin{equation*}
	\sum_{i=1}^m \underbrace{\frac{\pi_j  \mathcal{N} (\bm{x}_i|\bm{\mu}_j,\bm{\Sigma}_j)}{\sum_l \pi_l \mathcal{N} (\bm{x}_i|\bm{\mu}_l,\bm{\Sigma}_l)}}_{z_{ij}} \bm{\Sigma}_k (\bm{x}_i - \bm{\mu}_j)
	\end{equation*}
in which $z_{ij}=p(y_j=1|\bm{x}_i)$ is the posterior probability. Setting it to be zero leads to
	\begin{equation}
	\label{eq:GMM_mu}
	\bm{\mu}_j=\frac{\sum_{i=1}^m z_{ij} \bm{x}_i}{\sum_{i=1}^m z_{ij}}
	\end{equation}
Similarly,
	\begin{equation}
	\label{eq:GMM_sigma}
	\bm{\Sigma}_j=\frac{\sum_{i=1}^m z_{ij} (\bm{x}_i-\bm{\mu}_j)(\bm{x}_i-\bm{\mu}_j)^\top}{\sum_{i=1}^m z_{ij}}
	\end{equation}
	Then, take derivatives with regard to each $\pi_j$,
	\begin{equation*}
	\sum_{i=1}^m \frac{\mathcal{N} (\bm{x}_i|\bm{\mu}_j,\bm{\Sigma}_j) }{\sum_l \pi_l \mathcal{N} (\bm{x}_i|\bm{\mu}_l,\bm{\Sigma}_l)} + \lambda = \sum_{i=1}^m \frac{z_{ij}}{\pi_j} + \lambda
	\end{equation*}
which leads to $\pi_j=-\frac{\sum_{i=1}^m z_{ij}}{\lambda}$. With the constraint that $\sum_{j=1}^k\pi_j=-\sum_{i=1}^m\sum_{j=1}^k z_{ij}/\lambda = -m/\lambda=1$, then $\lambda=-m$, and hence

	\begin{equation}
	\label{eq:GMM_pi}
	\pi_j=\frac{\sum_{i=1}^m z_{ij}}{m}
	\end{equation}
	
	It means that the mixing coefficient for the $k$-th component is given by the average posterior which that component takes for explaining the data points. Notes that the calculation above drops into a circle form: $\bm{\mu}, \bm{\Sigma} \rightarrow z_{ij} \rightarrow \bm{\mu}, \bm{\Sigma}$, hence we must do it in an iterative way, which is the EM algorithm for GMM:
	
	\begin{itemize}
	\item fix $k$, the number of Gaussian components;
	\item assign each sample to each components with equal probability, \textit{i.e.}, $z_{ij}=\frac{1}{k},j=1,\cdots,k$ and $\pi_j=\frac{1}{k},j=1,\cdots,k$ also;
	\item M-step, solve $\bm{\mu}, \bm{\Sigma}$ according to \textit{Eq.}\ref{eq:GMM_mu} and \textit{Eq.}\ref{eq:GMM_sigma};
	\item E-step, solve $z_{ij},\pi_i$ according to \textit{Eq.}\ref{eq:GMM_pi}.
	\end{itemize}
	
	In \textit{Ex3.1}, we will see the relationship between GMM and K-means clustering algorithm.
	
\section{\textit{v.s.} discriminant models}
	\label{sec:final}
	In generative approaches, it is assumed that the underlying distribution over the data has a specific parametric form and the goal is to estimate the parameters of the model. But in discriminative approaches, the goal is rather to learn an accurate predictor directly. 
	
	Of course, if we succeed in learning the underlying distribution accurately, prediction from the Bayes optimal classifier is reliable. The problem is that, it is usually more difficult to learn the underlying distribution than to learn an accurate predictor. This was phrased by Vladimir Vapnik:
	\begin{center}
	\textit{"When solving a given problem, try to avoid a more general problem as an intermediate step."}
	\end{center}

	However, in some situations, it is reasonable to adopt the generative models. Sometimes it is easier (computationally) to estimate the parameters of the model than to learn a discriminative predictor. Additionally, in some cases we do not have a specific task at hand but rather would like to use the data at a later time.
	
	Modern generative models have another big goal, that is to 'generate' (sample from the underlying distribution) data like that in the real world. The intuition behind this approach follows a famous quote from Richard Feynman:	
	\begin{center}
	\textit{"What I cannot create, I do not understand."}
	\end{center}

	\subsection{Naive Bayes to linear discriminant models}
	The usual assumption in Naive Bayes classifier is that each conditional probability $p(X=\bm{x}|Y=y)$ is a Gaussian distribution. Consider the binary classification task, denote the two conditional distribution as $\mathcal{N}(\bm{\mu}_0,\bm{\Sigma}_0)$, $\mathcal{N}(\bm{\mu}_1,\bm{\Sigma}_1)$, we will predict $h_{\mathrm{Bayes}}(\bm{x})=1$ iff.
	
	\begin{equation*}
	\begin{split}
	&\frac{p(Y=0) p(X=\bm{x}|Y=0)}{p(Y=1) p(X=\bm{x}|Y=1)} > 1 \\
	&\iff \log \frac{p(Y=0)}{p(Y=1)} + \log p(X=\bm{x}|Y=0) - \log p(X=\bm{x}|Y=1) > 0 \\
	&\iff -\frac{1}{2}(\bm{x}-\bm{\mu}_0)^\top \bm{\Sigma}_0^{-1}(\bm{x}-\bm{\mu}_0) + \frac{1}{2}(\bm{x}-\bm{\mu}_1)^\top \bm{\Sigma}_1^{-1}(\bm{x}-\bm{\mu}_1) +  \frac{1}{2}\log\frac{|\bm{\Sigma}_1|}{|\bm{\Sigma}_0|} + \log \frac{p(Y=0)}{p(Y=1)} > 0 \\
	&\iff \frac{1}{2} \bm{x}^\top ( \bm{\Sigma}_1^{-1} - \bm{\Sigma}_0^{-1}) \bm{x} + (\bm{\mu}_0^\top\bm{\Sigma}_0^{-1} - \bm{\mu}_1^\top\bm{\Sigma}_1^{-1}) \bm{x} + \underbrace{\frac{1}{2} (\bm{\mu}_1^\top\bm{\Sigma}_1^{-1}\bm{\mu}_1 - \bm{\mu}_0^\top\bm{\Sigma}_0^{-1}\bm{\mu}_0 ) + \frac{1}{2}\log\frac{|\bm{\Sigma}_1|}{|\bm{\Sigma}_0|} + \log \frac{p(Y=0)}{p(Y=1)}}_{\mathrm{b}} > 0
	\end{split}
	\end{equation*}
which is a quadratic discriminant function.

	Further, if we assume that $\bm{\Sigma}_0=\bm{\Sigma}_1=\bm{\Sigma}$, the classifier can be simplified to be a linear discriminant function $\bm{w}\cdot\bm{x}+b$, with $\bm{w}=(\bm{\mu}_0 - \bm{\mu}_1)^\top\bm{\Sigma}^{-1}$ and $b=\frac{1}{2} (\bm{\mu}_1^\top\bm{\Sigma}^{-1}\bm{\mu}_1 - \bm{\mu}_0^\top\bm{\Sigma}^{-1}\bm{\mu}_0 ) + \log \frac{p(Y=0)}{p(Y=1)}$. If the prior probability is equal, namely $p(Y=0)=p(Y=1)$, the bias term can be further simplified.
	
\section{Exercises and solutions}
\begin{itemize}
\item[Ex3.1] K-means (see \textit{UML?, PRML?}), is a simple but important clustering algorithm. In fact, GMM is sometimes called \textit{soft} K-means. As a hard version, K-means assigns the most probable cluster label to an example (\textit{i.e.}, $z_ij=1$ for one of $j\in{1,\cdots,k}$ but 0 for others), and calculate the mean and covariance based on the in-cluster instead of global data.
\item[Ex3.2]

\end{itemize}
\textit{
	Chapter 4. Linear models for classification and regression, penalization \\
	  Chapter 5. Decision stumps, ensemble learning, Bayes PAC \\
      Chapter 6. Perceptron, MLP, deep learning, Generalization bounds on deep learning.}

\end{document}