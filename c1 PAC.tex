\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}

\author{Siheng Zhang\\zhangsiheng@cvte.com}
\title{Chapter ONE Probably Approximately Correct (PAC)}
\date{\today}      
\usepackage[a4paper,left=18mm,right=18mm,top=25mm,bottom=25mm]{geometry} 

\begin{document}
\maketitle  

The notes is mainly based on the following books:

\begin{itemize}
\item Understanding Machine Learning: From Theory to Algorithms, Shai Shalev-Shwartz and Shai Ben-David, 2014 \footnote{https://www.cs.huji.ac.il/\~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf}

\item pattern recognition and machine learning, Christopher M. Bishop, 2006 \footnote{http://users.isr.ist.utl.pt/\~wurmd/Livros/school/Bishop\ -\ Pattern\ Recognition\ And\ Machine\ Learning\ -\ Springer\ \ 2006.pdf}

\item Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Friedman, 2009 \footnote{https://mitpress.mit.edu/books/probabilistic-graphical-models}

\item Graphical Models, Exponential Families, and Variational Inference, Martin J. Wainwright and Michael I. Jordan, 2008 \footnote{https://people.eecs.berkeley.edu/\~wainwrig/Papers/WaiJor08\_FTML.pdf}
\end{itemize}

This part corresponds to \textbf{Chapter 2-5 in UML}, and mainly answers the following questions:

\begin{itemize}
\item What can we know about the generalization error?
\item How does the hypothesis set (in application, the choice of classifier/regressor or so on) reflect our prior knowledge, or, inductive bias?
\end{itemize}

\newpage

\tableofcontents
\newpage

\section{Formulation}

\subsection{The learner's input, output, and evaluation}

\begin{itemize}

	\item \textbf{input}:
	
	\begin{itemize}
	\item Domain set: instance $x \in \mathcal{X}$.
	\item Label set: label $y \in \mathcal{Y}$. Currently, just consider the binary classification task.
	\item Training set: $S=((x_1,y_1),\cdots,(x_m,y_m))$ is a finite sequence.
	\end{itemize}	 

	\item \textbf{output}: hypothesis (or classifier, regressor) $h:\mathcal{X}\rightarrow\mathcal{Y}$.

	\item \textbf{data generation model}: Assume that the instances are generated by some probability distribution $\mathcal{D}$, and there is some 'correct' labeling function (currently): $f:\mathcal{X}\rightarrow\mathcal{Y}$.
	
	The i.i.d. assumption: the training samples are independently and identically distributed.

    \textit{\underline{remark1}: The learner is blind to the data generation model.}

    \textit{\underline{remark2}: Usually called 'training set', but must be 'training sequence', because the same samples may repeat, and some training algorithms are order-sensitive.}
    
    \textit{\underline{remark3}: Strictly speaking, the distribution $\mathcal{D}$ is defined over $\mathcal{X}\times\mathcal{Y}$}.

	\item \textbf{Generalization error}: \textit{a.k.a}, true error/risk. Assign a number, $\mathcal{D}(A)$, which determines how likely it is to observe a point $x \in A \subset \mathcal{X}$.
	
	\begin{equation}
	L_{\mathcal{D},f}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{x\sim\mathcal{D}}[h(x)\neq f(x)]\overset{def}{=}\mathcal{D}(\{x:h(x)\neq f(x)\})
	\end{equation}
	
\end{itemize}

\section{From ERM to PAC}

\subsection{ERM (Empirical Risk Minimization) may lead to overfitting}

	Since the generalization error is intractable, turn to minimize the \textbf{empirical risk}:

	\begin{equation}
	L_S(h)\overset{def}{=}\frac{|\left\{(x_i,y_i)\in S:h(x_i)\neq y_i \right\}|}{m}
	\end{equation}

	Consider a 'lazy' learner $h$, which predict $y=y_i$ iff. $x=x_i$, and 0 otherwise. It has 1/2 probability to fail for unseen instances, i.e., $L_{\mathcal{D},f}(h)=1/2$, while $L_S(h)=0$. Hence, it is an excellent learner on the training set, but a poor learner in the universe case. This phenomenon is called 'overfitting'. The lesson behind this learner is: without restriction on the hypothesis set, ERM can lead to overfitting.

\subsection{ERM with restricted hypothesis set (inductive bias)}

	Instead of $h_S\in \arg\min L_S(h)$, ERM with restricted hypothesis set return the following hypothesis:
	
	\begin{equation}
	h_S\in \arg\min \limits_{h\in\mathcal{H}}L_S(h)
	\end{equation}

	Start from an ideal case, in which the \textbf{realizability assumption} holds, i.e., there exists $h^*\in\mathcal{H}$, such that $L_{\mathcal{D},f}(h^*) = 0$.
	
	It implies that $L_S(h^*)=0$, $L_S(h_S)=0$. However, we are interested in $L_{\mathcal{D},f}(h_S)$.

\subsection{PAC (Probably Approximately Correct) learnability}

	\textbf{Definition}: Training on $m\geq m_\mathcal{H}(\epsilon,\delta)$ samples, there exists an algorithm to be able to achieve \textbf{accuracy} at least $1-\epsilon$ with \textbf{confidence} at least $1-\delta$.
	
	\begin{theorem} Finite hypothesis classes are PAC learnable, and the sample complexity is: $m_\mathcal{H}(\epsilon,\delta)=\frac{\log(|\mathcal{H}|/\delta)}{\epsilon}$.
	\end{theorem}
	
	\begin{proof}
	Let $\mathcal{H}_B$ be the set of 'bad' hypothesis, that is, $\mathcal{H}_B\subset\mathcal{H}$, and $\forall h\in\mathcal{H}_B,L_{\mathcal{D},f}(h)>\epsilon$. Let $M$ be the set of 'misleading' samples, that is $M=\{S:\exists h\in \mathcal{H}_B, L_S(h)=0\}$. Note that, 
	
	\begin{equation*}
	M=\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\}
	\end{equation*}
	
	The goal is to bound the probability of the event $L_{\mathcal{D},f}(h_S)>\epsilon$,
	\begin{equation*}
	\begin{split}
	&\mathcal{D}^m(\{S:L_{\mathcal{D},f}(h_S)>\epsilon\})\leq\mathcal{D}^m(M) \\
	&=\mathcal{D}^m(\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\})         
	=\sum_{h\in\mathcal{H}_B}\prod_{i=1}^m\mathcal{D}(\{x_i:f(x_i)=h(x_i)\})  \\
	&\overset{i.i.d.}{=}\sum_{h\in\mathcal{H}_B}(1-L_{\mathcal{D},f}(h))^m   
	\leq\sum_{h\in\mathcal{H}_B}(1-\epsilon)^m\leq\sum_{h\in\mathcal{H}_B}\exp(-\epsilon m) \\
	&\leq|\mathcal{H}|\exp(-\epsilon m)
	\end{split}
	\end{equation*}

	Let $|\mathcal{H}|\exp(-\epsilon m)\leq\delta$, we can solve that $m\geq\log(|\mathcal{H}|/\delta)/\epsilon$.
	\end{proof}

\subsection{No-Free-Lunch}

	\begin{theorem}
	Let $A$ be any learning algorithm for the task of binary classification with respect to the 0-1 loss over a domain $\mathcal{X}$. Let $m$ be any number smaller than $\mathcal{X}/2$, representing a training set size. Then, there exists a distribution $\mathcal{D}$ over ${X}\times\{0,1\}$ such that:
	
	\begin{itemize}
	\item There exists a function $f:\mathcal{X}\rightarrow\{0,1\}$ with $L_\mathcal{D}(f)=0$.
	\item With probability of at least $1/7$ over the choice of $S\sim\mathcal{D}^m$ we have that $L_\mathcal{D}(A(S))\geq 1/8$.
	\end{itemize} 
	\end{theorem}
	
	\begin{proof}
	Let $C\subseteq\mathcal{X}$ of size $2m$. There are $T=2^{2m}$ possible functions $f_1,\cdots,f_T$ defined on $C\rightarrow\{0,1\}$. For each such function, let $\mathcal{D}_i$ be a distribution over $\mathcal{C}\times\{0,1\}$ defined as follow:
	
	\begin{equation*}
	\mathcal{D}_i(\{(x,y)\})=\left\{
	\begin{matrix} 1/|C|,& \mathrm{if}\ y=f_i(x) \\ 0,& \mathrm{otherwise} 
	\end{matrix}\right.
	\end{equation*}
	
	There are $k=(2m)^m$ possible sequences $S_1,\cdots,S_k$, each of $m$ examples from $C$. Also, we denote the sequence $\mathcal{S}_j$ labelled by the function $f_i$ as $\mathcal{S}_j^i$. If the distribution is $\mathcal{D}_i$, then the possible training sets are $S_1^i,\cdots,S_k^i$, with equal probability. Therefore,

	\begin{equation*}
	\mathop{\mathbb{E}}\limits_{S\sim\mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]=\frac{1}{k}\sum_{j=1}^kL_{\mathcal{D}_i}(A(S_j^i))
	\end{equation*}
	
	Using the facts that 'maximum' is larger than 'average', and that 'average' is larger than 'minimum', we have
	
	\begin{equation*}
	\max_{i\in\{1,\cdots,T\}} \frac{1}{k}\sum_{j=1}^k L_{\mathcal{D}_i}(A(S_j^i)) \geq \frac{1}{T}\sum_{i=1}^T \frac{1}{k} \sum_{j=1}^k L_{\mathcal{D}_i}(A(S_j^i)) \geq \min_{j\in\{1,\cdots,k\}}\frac{1}{T}\sum_{i=1}^T L_{\mathcal{D}_i} (A(S_j^i))
	\end{equation*}

	Next, fix some $j\in\{1,\cdots,k\}$. Denote $S_j=(x_1,\cdots,x_m)$ and let $v_1,\cdots,v_p$ be the examples in $C$ that do not appear in $S_j$. Clearly, $p\geq m$. Therefore, for every $h:C\rightarrow\{0,1\}$,
	
	\begin{equation*}
	L_{\mathcal{D}_i}(h)=\frac{1}{2m}\sum_{x\in C}\mathbb{I}_{[h(x)\neq f_i(x)]}\geq\frac{1}{2p}\sum_{r=1}^p\mathbb{I}_{[h(v_r)\neq f_i(v_r)]}
	\end{equation*}
and hence,

	\begin{equation*}
	\frac{1}{T}\sum_{i=1}^T L_{\mathcal{D}_i} (A(S_j^i))\geq\frac{1}{2}\min_{r\in\{1,\cdots,p\}}\frac{1}{T}\mathbb{I}_{[A(S^i_j)(v_r)\neq f_i(v_r)]}
	\end{equation*}

	Next, fix some $r\in[p]$, We can partition all the functions $f_1,\cdots,f_T$ into $T/2$ disjoint pairs, where for a pair $(f_i,f_{i'})$ we have that $\forall c\in C,f_i(c)\neq f_{i'}(c)$ iff. $c=v_r$. Since for such a pair we must have $S_j^i=S_j^{i'}$, it follows that $\mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]}+\mathbb{I}_{[A(S_j^{i'})(v_r)\neq f_{i'}(v_r)]}=1$, which yields:
	
	\begin{equation*}
	\frac{1}{T}\sum_{i=1}^T\mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]}=\frac{1}{2}
	\end{equation*}

	In conclusion, it holds that

	\begin{equation*}
	\max_{i\in\{1,\cdots,T\}}\mathop{\mathbb{E}}_{S\sim\mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]\geq 1/4
	\end{equation*}

	This means that for every algorithm, there exists $f,\mathcal{D}$, such that $L_\mathcal{D}(f)=0$ and $\mathop{\mathbb{E}}\limits_{S\sim\mathcal{D}^m}[L_\mathcal{D}(A(S))]\geq 1/4$.

	\textit{(following is part of UML Ex5.1)} For a random variable $\theta\in[0,1]$ such that $\mathbb{E}(\theta)\geq 1/4$, we have:
	
	\begin{equation*}
	p\left(\theta\geq\frac{1}{8}\right)=\int_\frac{1}{8}^1 p(\theta) \mathrm{d}\theta \geq\int_\frac{1}{8}^1 \theta p(\theta) \mathrm{d}\theta=\mathbb{E}(\theta)-\int_0^\frac{1}{8}\theta p(\theta)\mathrm{d}\theta \geq\mathbb{E}(\theta)-\frac{1}{8}\int_0^\frac{1}{8} p(\theta)\mathrm{d}\theta = \frac{1}{4} - \frac{1}{8}\left( 1-\int^1_\frac{1}{8}p(\theta) \mathrm{d}\theta \right)
	\end{equation*}
which leads to $p(\theta\geq 1/8)\geq 1/7$.
	\end{proof}

	NFL theorem tells the neccessity of inductive bias. Philosophically, if someone can explain every phenomenon, his explanations are worthless.
	
\subsection{Agnostic PAC}
	\subsubsection{Beyond realizability assumption}
	In practical, the 'true' labelling function may not exist, and the labels may not be fully determined by the features on hand. Then Agnostic PAC learnability is defined as: training on $m\geq m_\mathcal{H}(\epsilon,\delta)$ samples, there exists an algorithm with \textbf{confidence} at least $1-\delta$ to achieve that:
	
	\begin{equation*}
	L_\mathcal{D}(h)\leq\min\limits_{h'\in\mathcal{H}}L_\mathcal{D}(h')+\epsilon
	\end{equation*}
in which $L_\mathcal{D}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{(x,y)\sim\mathcal{D}}[h(x)\neq y]\overset{def}{=}\mathcal{D}(\{x:h(x)\neq y\})$.

	\subsubsection{Beyond binary classification}

	
	Agnostic PAC learnability remains the same with:
	
	\begin{equation}
	\mathcal{D}(h)=\mathop{\mathbb{E}}\limits_{x\sim\mathcal{D}}[l(h,z)]
	\end{equation}
in which $l(\cdot)$ is 0-1 loss for multiclass classification and square loss for regression. 

	\subsubsection{Sample complexity under Agn-PAC: via uniform convergence}
	
	\textbf{Definition $\epsilon$-representative}: A training set S is called $\epsilon$-representative if $\forall h\in\mathcal{H},|L_S(h)-L_\mathcal{D}(h)|\leq\epsilon$.
	
	\begin{theorem}
	ERM rule is suitable for $\epsilon/2$-representative samples.
	\end{theorem}
	
	\begin{proof}
	for every $h\in\mathcal{H}$,
	
	\begin{equation}
	L_\mathcal{D}(h_S)\leq L_S(h)+\frac{\epsilon}{2}\leq L_S(h)+\frac{\epsilon}{2}\leq L_S(h)+\epsilon
	\end{equation}	 

	Hence, $L_\mathcal{D}(h_S)\leq \min_{h\in\mathcal{H}}L_S(h)+\epsilon$.
	\end{proof}

	\begin{theorem}
	\textbf{Agnostic PAC sample complexity} Assume that the range of the loss function is <$[0,1]$, or more general, $[a,b]$, then a finite hypothesis set $\mathcal{H}$ enjoys the agnostic PAC learnability with sample complexity 
	
	\begin{equation}
	m_\mathcal{H}(\epsilon,\delta)\leq m^{UC}_\mathcal{H}(\epsilon/2,\delta)\leq\left\lceil\frac{2\log(2|\mathcal{H}|/\delta)(b-a)^2}{\epsilon^2}\right\rceil
	\end{equation}
	\end{theorem}

\section{Error decomposition}

	\begin{equation}
	\begin{split}
	L_\mathcal{D}(h_S) &= \epsilon_{\mathrm{app}}+\epsilon_{\mathrm{est}} \\
	\epsilon_{\mathrm{app}} &= \min\limits_{h\in\mathcal{H}}L_\mathcal{D}(h) \\
	\epsilon_{\mathrm{est}} &= L_\mathcal{D}(h_S)-\epsilon_{\mathrm{app}}
	\end{split}
	\end{equation}		

	\begin{itemize}
	\item \textbf{The Approximation Error} measures how much risk we have because we restrict ourselves to a specific class, namely, how much *inductive bias* we have. The approximation error does not depend on the sample size and is determined by the hypothesis class chosen. Enlarging the hypothesis class can decrease the approximation error.
	\item \textbf{The Estimation Error} measures the empirical risk (i.e., training error), which is only an estimate of the true risk. The quality of this estimation depends on the training set size (decreases with it) and on the size, or complexity, of the hypothesis class (logarithmically increases with it).
	\end{itemize}

\section{Summary}

Now that, we have come to some important conclusions under the PAC learning framework:

\begin{enumerate}
\item No universal learner;
\item Inductive bias is neccessary to avoid overfitting;
\item Sample complexity is function about hypothesis set, confidence level and error, interestingly, it is nothing to do with the dimension of feature space;
\item Inductive bias controls the balance of approximation error and estimation error.
\end{enumerate}

	We have reached the fundamental question in learning theory: \textbf{Over which hypothesis classes, ERM learning will not result in overfitting (or, PAC learnable)?} Currently, we just confirm the PAC learnability for finite classes. In the next chapter, the most important part in learning theory, VC-dimension, will gives a more precise answer.


\section{Excercises and solutions}

\begin{itemize}
\item[Ex1] (UML Ex2.2) Let $\mathcal{H}$ be a class of binary classifiers over a domain $\mathcal{X}$. Let $\mathcal{D}$ be an unknown distribution over $\mathcal{X}$, and let $f$ be the target hypothesis in $\mathcal{H}$. Fix some $h\in\mathcal{H}$, show that the expected value of $L_S(h)$ over the choice of $S$ equals $L_{\mathcal{D},f}(h)$, namely,

\begin{equation*}
\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=L_{\mathcal{D},f}(h)
\end{equation*}

\item[] \textbf{Solution}: according to the definition, 
\begin{equation*}
\begin{split}
\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)] 
&= \sum_{S} \mathcal{D}^m(S)\frac{|\{(x_i,y_i)\in S:h(x_i)\neq y_i\}|}{m} \\
&= \sum_S\mathcal{D}\{(x_i,y_i)\in S:h(x_i)\neq y_i\} \\
&= \mathcal{D}(\{x:h(x)\neq f(x)\}) = L_{\mathcal{D},f}(h)
\end{split}
\end{equation*}

\item[Ex2] (UML Ex2.3) \textbf{Axis Aligned rectangles}: An axis aligned rectangle classifier in the plane is a classifier that assigns the value 1 to a point if and only if it is inside a certain rectangle. Formally, given real numbers $a_1\leq b_1,a_2\leq b_2$, define the classifier $h(a_1,b_1,a_2,b_2)$ by:

	\begin{equation*}
	h(a_1,b_1,a_2,b_2)(x_1, x_2) = \left\{\begin{matrix}
	1,& \mathrm{if}\ a_1 \leq x_1 \leq b1\ \mathrm{and}\  a_2 \leq x_2 \leq b_2 \\
	0,& \mathrm{otherwise}
	\end{matrix}\right.
	\end{equation*}

	The class of all axis aligned rectangles in the plane is defined as:
	
	\begin{equation*}
	\mathcal{H}^2_{\mathrm{rec}}=\{h_{(a_1,b_1,a_2,b_2)}:a_1\leq b_1,\ \mathrm{and}\ a_2\leq b_2\}
	\end{equation*}
Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

	\begin{itemize}
	\item[2.1] Let $A$ be the algorithm that returns the smallest rectangle enclosing all positive examples in the training set. Show that $A$ is an ERM.
	\item[2.2] Show that if $A$ receives a training set of size $\geq 4\frac{\log(4/\delta)}{\epsilon}$, then, with probability of at least $1-\delta$ it returns a hypothesis with error of at most $\epsilon$.
	
	\textit{Hint: Fix some distribution $\mathcal{D}$ over $\mathcal{X}$, let $R^*=R(a^*_1,b^*_1,a^*_2,b^*_2)$ be the rectangle that generates the labels, and let $f$ be the corresponding hypothesis. Let $a_1\geq a^*_1$ be a number such that the probability mass (with respect to $\mathcal{D}$) of the rectangle $R_1=R(a^*_1,a_1,a^*_2,b^*_2)$ is exactly $\epsilon/4$. Similarly, let $b_1,a_2,b_2$ be numbers such that the probability masses of the rectangles $R_2=R(b_1,b^*_1,a^*_2,b^*_2)$, $R_3=R(a^*_1,b^*_1,a^*_2,a_2)$, $R_4=R(a^*_1,b^*_1,b_2,b^*_2)$ are all $\epsilon/4$. Let $R(S)$ be the rectangle returned by $A$. See illustration in the following figure.}
	\begin{itemize}
	\item Show that $R(S)\subseteq R^*$.
	\item Show that if $S$ contains (positive) examples in all of the rectangles $R_1,R_2,R_3,R_4$, then the hypothesis returned by $A$ has error of at most $\epsilon$.
	\item For each $i\in\{1,\cdots,4\}$, upper bound the probability that $S$ does not contain an example from $R_i$.
	\item Use the union bound to conclude the argument.
	\end{itemize}
	 
	\item[2.3] Repeat the previous question for the class of axis aligned rectangles in $\mathbb{R}^d$.
		
	\item[2.4] Show that the runtime of applying the algorithm $A$ mentioned earlier is polynomial in $d, 1/\epsilon$, and in $\log(1/\delta)$.
	\end{itemize}

	\item[] \textbf{Solution}
	
	\begin{itemize}
	\item[2.1] In realizable setup, since the tightest rectangle enclosing all positive example is returned, all positive and negative instances are correctly classified.
	\end{itemize}

\item[Ex3] (UML Ex3.2) Let $\mathcal{X}$ be a discrete domain, and let $\mathcal{H}_{\text{Singleton}}=\{h_z:z\in\mathcal{X}\}\cup\{h^-\}$, where for each $z\in\mathcal{X}$, $h_z$ is the function defined by $h_z(x)=1$ if $x=z$ and $h_z(x)=0$ if $x\neq z$. $h^-$ is simply the all-negative hypothesis, namely, $\forall x\in\mathcal{X},h^-(x)=0$. The realizability assumption here implies that the true hypothesis $f$ labels negatively all examples in the domain, perhaps except one.

	\begin{itemize}
	\item[3.1] Describe an algorithm that implements the ERM rule for learning $\mathcal{H}_{\text{Singleton}}$ in the realizable setup.
	\item[3.2] Show that $\mathcal{H}_{\text{Singleton}}$ is PAC learnable. Provide an upper bound on the sample complexity.
	\end{itemize} 

	\item[] \textbf{Solution}:
	\begin{itemize}
	\item[3.1] Traverse $z\in\mathcal{X}$ then output $h_z$ or $h^-$.
	\item[3.2] If for any $i\in[1,\cdots,m]$, $h_{x_i}$ is the true hypothesis, the algorithm can find it in the realizable setup. Otherwise, the algorithm outputs $h^-$, which can be either true or false (i.e., the target $z^*$ is not in the training set). Note that in the second case, the algorithm only makes a single error when generalize to all cases, and hence $p(z^*)\geq\epsilon$ (otherwise, it is meaningless),
	
	\begin{equation*}
	P(L_{\mathcal{D},f}(h_S)>\epsilon)\leq(1-p(z^*))^m\leq(1-\epsilon)^m\leq\exp(-\epsilon m)\leq\delta
	\end{equation*}

which leads to
	\begin{equation*}
	m_\mathcal{H}(\epsilon,\delta)\leq\left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil
	\end{equation*}
	

\item[Ex4] (UML Ex3.3) Let $\mathcal{X}=\mathbb{R}^2$, $\mathcal{Y}=\{0,1\}$, and let $\mathcal{H}$ be the class of concentric circles in the plane, that is, $\mathcal{H}=\{h_r:r\in\mathbb{R}_+\}$, where $h_r(x)=\mathbb{I}_{[\| x\|\leq r]}$. Prove that $\mathcal{H}$ is PAC learnable (assume realizability), and its sample complexity is bounded by:

	\begin{equation*}
	m_\mathcal{H}(\epsilon,\delta)\leq\left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil
	\end{equation*}
	
	\item[] \textbf{Solution}
	
\item[Ex5] (UML Ex3.4)

\item[Ex6] (UML Ex3.7) The Bayes optimal predictor: Show that for every probability distribution $\mathcal{D}$, the Bayes optimal predictor $f_\mathcal{D}$ is optimal, in the sense that for every classifier $g$ from $X$ to $\{0, 1\}$, $L_\mathcal{D}(f_{\mathcal{D}}) \leq L_\mathcal{D}(g)$.

\item[] \textbf{Solution}: The Bayes predictor labels a sample $x$ according to

	\begin{equation*}
	f_\mathcal{D}(x) = \left\{
	\begin{matrix} 0,& \mathrm{if}\ \mathcal{D}((x,0))\geq \mathcal{D}((x,1)) \\ 1,& \mathrm{otherwise} 
	\end{matrix}\right.
	\end{equation*}
	
	When it labels a sample to be class 0, it holds that $\mathcal{D}((x,0))\geq \mathcal{D}((x,1))$. If the true label function also makes such a decision, then Bayes predictor makes no error. Otherwise, $f(x)=1$, but its probability is no more than 1/2. Any other classifier that labels $x$ to be class 1 will suffer a risk no less than 1/2. Hence, in total, Bayes predictor is the optimal.

\item[Ex7] (UML Ex3.9) Consider a variant of the PAC model in which there are two example oracles: one that generates positive examples and one that generates negative examples, both according to the underlying distribution $\mathcal{D} on \mathcal{X}$. Formally, given a target function $f: \mathcal{X}\rightarrow\{0,1\}$, let $\mathcal{D}^+ be the distribution over \mathcal{X}^+ = \{x\in\mathcal{X}: f(x)= 1\}$ defined by $\mathcal{D}^+(A) = \mathcal{D}(A)/\mathcal{D}(\mathcal{X}^+)$, for every $A\in\mathcal{X}^+$. Similarly, $\mathcal{D}^-$ is the distribution over $\mathcal{X}^-$ induced by $\mathcal{D}$. 
	
	The definition of PAC learnability in the two-oracle model is the same as the standard definition of PAC learnability except that here the learner has access to $m_\mathcal{H}^+(\epsilon, \delta)$  i.i.d. examples from $\mathcal{D}^+$ and $m^-(\epsilon, \delta)$ i.i.d. examples from $\mathcal{D}^-$. The learner's goal is to output $h$ s.t. with probability at least $1-\delta$ (over the choice of the two training sets, and possibly over the nondeterministic decisions made by the learning algorithm), both $L(\mathcal{D}^+, f)(h)\leq\epsilon$ and $L(\mathcal{D}^-, f)(h)\leq\epsilon$.
	\begin{itemize}
	\item[7.1] Show that if $H$ is PAC learnable (in the standard one-oracle model), then $H$ is PAC learnable in the two-oracle model.
	\item[7.2] Define $h^+$ to be the always-plus hypothesis and $h^-$ to be the always minus hypothesis. Assume that $h^+, h^-\in \mathcal{H}$. Show that if $\mathcal{H}$ is PAC learnable in the two-oracle model, then $\mathcal{H}$ is PAC learnable in the standard one-oracle model.
	\end{itemize}
	
\item[Ex8] (UML Ex5.3) Prove that if $|\mathcal{X}|\geq km$ for a positive integer $k\geq 2$, then we can replace the lower bound in the No-Free-Lunch theorem. Namely, for the task of binary classification, there exists a distribution $\mathcal{D}\sim\mathcal{X}\times\{0,1\}$ such that:

	\begin{itemize}
	\item[8.1] There exists a function $f:\mathcal{X}\rightarrow\{0,1\}$ with $L_\mathcal{D}(f)=0$.
	\item[8.2] $\mathbb{E}_{S\sim\mathcal{D}^m}[L_\mathcal{D}(A(S))]\geq\frac{1}{2}-\frac{1}{2k}$.
	\end{itemize}

\item[] \textbf{Solution}:
	Only the second proposition should be proved. Similar with the proof in above, 
	\begin{equation*}
	L_{\mathcal{D}_i}(h)=\frac{1}{km}\sum_{x\in C}\mathbb{I}_{[h(x)\neq f_i(x)]}\geq\frac{1}{km}\sum_{r=1}^p\mathbb{I}_{[h(v_r)\neq f_i(v_r)]}\geq\frac{k-1}{pk}\sum_{r=1}^p\mathbb{I}_{[h(v_r)\neq f_i(v_r)]}
	\end{equation*}
 
And similarity,

	\begin{equation*}
	\frac{1}{T}\sum_{i=1}^T L_{\mathcal{D}_i} (A(S_j^i))\geq\frac{k-1}{k}\min_{r\in\{1,\cdots,p\}}\frac{1}{T}\mathbb{I}_{[A(S^i_j)(v_r)\neq f_i(v_r)]}
	\end{equation*}
	
	So the final bound is $1/2-1/2k$.
\end{itemize} 

\end{itemize}
\textit{
      To be continue...\\
      Chapter 2. VC-dimension\\
      Chapter 3. Bayesian-PAC\\
      Chapter 4. Generalization in Deep Learning}

\end{document}