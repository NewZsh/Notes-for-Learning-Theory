\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}

\author{Siheng Zhang\\zhangsiheng@cvte.com}
\title{Chapter ONE Probably Approximately Correct (PAC)}
\date{\today}      
\usepackage[a4paper,left=18mm,right=18mm,top=25mm,bottom=25mm]{geometry} 

\begin{document}
\maketitle  

*The notes is mainly based on the following book*

\begin{itemize}
\item Understanding Machine Learning: From Theory to Algorithms, Shai Shalev-Shwartz and Shai Ben-David, 2014 \footnote{https://www.cs.huji.ac.il/\~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf}

\item pattern recognition and machine learning, Christopher M. Bishop, 2006 \footnote{http://users.isr.ist.utl.pt/\~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}

\item Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Friedman, 2009 \footnote{https://mitpress.mit.edu/books/probabilistic-graphical-models}

\item Graphical Models, Exponential Families, and Variational Inference, Martin J. Wainwright and Michael I. Jordan, 2008 \footnote{https://people.eecs.berkeley.edu/\~wainwrig/Papers/WaiJor08\_FTML.pdf}
\end{itemize}


\textit{Corresponding to Chapter 2-5 in UML.}

This part mainly answers the following questions:

\begin{itemize}
\item What can we know about the generalization error?
\item How does the hypothesis set (in application, the choice of classifier/regressor or so on) reflect our prior knowledge, or, inductive bias?
\end{itemize}

\newpage

\tableofcontents
\newpage

\section{Formulation}

\subsection{The learner's input, output, and evaluation}

\begin{itemize}

	\item \textbf{input}:
	
	\begin{itemize}
	\item Domain Set: instance $x \in \mathcal{X}$.
	\item Label Set: label $y \in \mathcal{Y}$. Currently, just consider the binary classification task.
	\item Training data: $S=((x_1,y_1),\cdots,(x_m,y_m))$ is a finite sequence.
	\end{itemize}	 

	\item \textbf{output}: hypothesis (or classifier, regressor) $h:\mathcal{X}\rightarrow\mathcal{Y}$.

	\item \textbf{data generation model}: Assume that the instances are generated by some probability distribution $\mathcal{D}$, and there is some 'correct' labeling function (currently): $f:\mathcal{X}\rightarrow\mathcal{Y}$.
	
	The i.i.d. assumption: the training samples are independently and identically distributed.

    \textit{\underline{remark1}: The learner is blind to the data generation model.}

    \textit{\underline{remark2}: Usually called 'training set', but must be 'training sequence', because the same sample may repeat, and some training algorithms is order-sensitive.}
    
    \textit{\underline{remark3}: Strictly speaking, the distribution $\mathcal{D}$ is defined over $\mathcal{X}\times\mathcal{Y}$}.

	\item \textbf{Generalization error}: \textit{a.k.a}, true error/risk.
	
	\begin{equation}
	L_{\mathcal{D},f}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{x\sim\mathcal{D}}[h(x)\neq f(x)]\overset{def}{=}\mathcal{D}({x:h(x)\neq f(x)})
	\end{equation}
	
\end{itemize}

\section{From ERM to PAC}

\subsection{ERM (Empirical Risk Minimization) may lead to overfitting}

	Since the generalization error is intractable, turn to minimize the \textbf{empirical risk}:

	\begin{equation}
	L_S(h)\overset{def}{=}\frac{|\left\{(x_i,y_i)\in S:h(x_i)\neq y_i \right\}|}{m}
	\end{equation}

	Consider a 'lazy' learner $h$, which predict $y=y_i$ iff. $x=x_i$, and 0 otherwise. It has 1/2 probability to fail for unseen instances, i.e., $L_{\mathcal{D},f}(h)=1/2$, while $L_S(h)=0$. Hence, it is an excellent learner on the training set, but a poor learner in the universe case. This phenomenon is called 'overfitting'. The lesson behind this learner is: without restriction on the hypothesis set, ERM can lead to overfitting.

\subsection{ERM with restricted hypothesis set (inductive bias)}

	Instead of $h_S\in \arg\min L_S(h)$, ERM with restricted hypothesis set return the following hypothesis:
	
	\begin{equation}
	h_S\in \arg\min \limits_{h\in\mathcal{H}}L_S(h)
	\end{equation}

	Start from an ideal case, in which the \textbf{realizability assumption} holds, i.e., there exists $h^*\in\mathcal{H}$, such that $L_{\mathcal{D},f}(h^*) = 0$.
	
	It implies that $L_S(h^*)=0$, $L_S(h_S)=0$. However, we are interested in $L_{\mathcal{D},f}(h_S)$.

\subsection{PAC (Probably Approximately Correct) learnability}

	\textbf{Definition}: Training on $m\geq m_\mathcal{H}(\epsilon,\delta)$ samples, there exists an algorithm to be able to achieve \textbf{accuracy} at least $1-\epsilon$ with \textbf{confidence} at least $1-\delta$.
	
	\begin{theorem} Finite hypothesis classes are PAC learnable, and the sample complexity is: $m_\mathcal{H}(\epsilon,\delta)=\frac{\log(|\mathcal{H}|/\delta)}{\epsilon}$.
	\end{theorem}
	
	\begin{proof}
	Let $\mathcal{H}_B$ be the set of 'bad' hypothesis, that is, $\mathcal{H}_B\subset\mathcal{H}$, and $\forall h\in\mathcal{H}_B,L_{\mathcal{D},f}(h)>\epsilon$. Let $M$ be the set of 'misleading' samples, that is $M=\{S:\exists h\in \mathcal{H}_B, L_S(h)=0\}$. Note that, 
	
	\begin{equation}
	M=\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\}
	\end{equation}
	
	The goal is to bound the probability of the event $L_{\mathcal{D},f}(h_S)>\epsilon$,
	\begin{equation}
	\begin{split}
	&\mathcal{D}^m(\{S:L_{\mathcal{D},f}(h_S)>\epsilon\})\leq\mathcal{D}^m(M) \\
	&=\mathcal{D}^m(\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\})         
	=\sum_{h\in\mathcal{H}_B}\prod_{i=1}^m\mathcal{D}(\{x_i:f(x_i)=h(x_i)\})  \\
	&\overset{i.i.d.}{=}\sum_{h\in\mathcal{H}_B}(1-L_{\mathcal{D},f}(h))^m   
	\leq\sum_{h\in\mathcal{H}_B}(1-\epsilon)^m\leq\sum_{h\in\mathcal{H}_B}\exp(-\epsilon m) \\
	&\leq|\mathcal{H}|\exp(-\epsilon m)
	\end{split}
	\end{equation}

	Let $|\mathcal{H}|\exp(-\epsilon m)\leq\delta$, we can solve that $m\geq\log(|\mathcal{H}|/\delta)/\epsilon$.
	\end{proof}

\subsection{No-Free-Lunch}

	\begin{theorem}
	Let $A$ be any learning algorithm for the task of binary classification with respect to the 0-1 loss over a domain $\mathcal{X}$. Let $m$ be any number smaller than $\mathcal{X}/2$, representing a training set size. Then, there exists a distribution $\mathcal{D}$ over ${X}\times\{0,1\}$ such that:
	
	\begin{itemize}
	\item There exists a function $f:\mathcal{X}\rightarrow\{0,1\}$ with $L_\mathcal{D}(f)=0$.
	\item With probability of at least 1/7 over the choice of $S\sim\mathcal{D}^m$ we have that $L_\mathcal{D}(A(S))\geq 1/8$.
	\end{itemize} 
	\end{theorem}
%*Proof*: Let <img src=http://latex.codecogs.com/gif.latex?C\subseteq\mathcal{X}> of size <img src=http://latex.codecogs.com/gif.latex?2m>. There are <img src=http://latex.codecogs.com/gif.latex?T%3D2^{2m}> possible functions <img src=http://latex.codecogs.com/gif.latex?C\rightarrow\{0,1\}>. Denote these functions by <img src=http://latex.codecogs.com/gif.latex?f_1,\cdots,f_T>. For each such function, let <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}_i> be a distribution over <img src=http://latex.codecogs.com/gif.latex?\mathcal{C}\times\{0,1\}> defined by
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BD%7D_i%28%5C%7B%28x%2Cy%29%5C%7D%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%201/C%26%5Ctext%7B%20if%20%7D%20y%3Df_i%28x%29%20%5C%5C%200%2C%26%5Ctext%7Botherwise%7D%20%5Cend%7Bmatrix%7D%5Cright.>
%</div align=center>
%
%There are <img src=http://latex.codecogs.com/gif.latex?k%3D(2m)^m> possible sequences of <img src=http://latex.codecogs.com/gif.latex?m> examples from <img src=http://latex.codecogs.com/gif.latex?C>. Denote these sequences by <img src=http://latex.codecogs.com/gif.latex?S_1,\cdots,S_k>. Also, we denote the sequence <img src=http://latex.codecogs.com/gif.latex?\mathca{S}_j> labeled by the function <img src=http://latex.codecogs.com/gif.latex?f_i> as <img src=http://latex.codecogs.com/gif.latex?\mathcal{S}_j^i>. If the distribuction is <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}_i>, then the possible training sets are <img src=http://latex.codecogs.com/gif.latex?S_1^i,\cdots,S_k^i> (with equal probability). Therefore,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\mathop{\mathbb{E}}\limits_{S\sim\mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]%3D\frac{1}{k}\sum_{j%3D1}^kL_{\mathcal{D}_i}(A(S_j^i))>
%</div align=center>
%
%Using the facts that 'maximum' is larger than 'average', and that 'average' is larger than 'minimum', we have
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?%5Cmax_%7Bi%5Cin%5C%7B1%2C%5Ccdots%2CT%5C%7D%7D%20%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bj%3D1%7D%5Ek%20L_%7B%5Cmathcal%7BD%7D_i%7D%28A%28S_j%5Ei%29%29%20%5Cgeq%20%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bi%3D1%7D%5ET%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_%7Bj%3D1%7D%5Ek%20L_%7B%5Cmathcal%7BD%7D_i%7D%28A%28S_j%5Ei%29%29%20%5Cgeq%20%5Cmin_%7Bj%5Cin%5C%7B1%2C%5Ccdots%2Ck%5C%7D%7D%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bi%3D1%7D%5ET%20L_%7B%5Cmathcal%7BD%7D_i%7D%20%28A%28S_j%5Ei%29%29>
%</div align=center>
%
%Next, fix some <img src=http://latex.codecogs.com/gif.latex?j\in\{1,\cdots,k\}>. Denote <img src=http://latex.codecogs.com/gif.latex?S_j%3D(x_1,\cdots,x_m)> and let <img src=http://latex.codecogs.com/gif.latex?v_1,\cdots,v_p> be the examples in <img src=http://latex.codecogs.com/gif.latex?C> that do not appear in <img src=http://latex.codecogs.com/gif.latex?S_j>. Clearly, <img src=http://latex.codecogs.com/gif.latex?p\geq%20m>. Therefore, for every <img src=http://latex.codecogs.com/gif.latex?h:C\rightarrow\{0,1\}>,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D}_i}(h)%3D\frac{1}{2m}\sum_{x\in%20C}\mathbb{I}_{[h(x)\neq%20f_i(x)]}\geq\frac{1}{2p}\sum_{r%3D1}^p\mathbb{I}_{[h(v_r)\neq%20f_i(v_r)]}>
%</div align=center>
%
%and hence,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bi%3D1%7D%5ET%20L_%7B%5Cmathcal%7BD%7D_i%7D%20%28A%28S_j%5Ei%29%29\geq\frac{1}{2}\min_{r\in\{1,\cdots,p\}}\frac{1}{T}\mathbb{I}_{[A(S^i_j)(v_r)\neq%20f_i(v_r)]}>
%</div align=center>
%
%Next, fix some <img src=http://latex.codecogs.com/gif.latex?r\in[p]>, We can partition all the functions <img src=http://latex.codecogs.com/gif.latex?f_1,\cdots,f_T> into <img src=http://latex.codecogs.com/gif.latex?T/2> disjoint pairs, where for a pair <img src=http://latex.codecogs.com/gif.latex?(f_i,f_{i%27})> we have that <img src=http://latex.codecogs.com/gif.latex?\forall%20c\in%20C,f_i(c)\neq%20f_{i%27}(c)> iff. <img src=http://latex.codecogs.com/gif.latex?c%3Dv_r>. Since for such a pair we must have <img src=http://latex.codecogs.com/gif.latex?S_j^i%3DS_j^{i%27}> it follows that <img src=http://latex.codecogs.com/gif.latex?\mathbb{I}_{[A(S_j^i)(v_r)\neq%20f_i(v_r)]}+\mathbb{I}_{[A(S_j^{i%27})(v_r)\neq%20f_{i%27}(v_r)]}%3D1>, which yields
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\frac{1}{T}\sum_{i%3D1}^T\mathbb{I}_{[A(S_j^i)(v_r)\neq%20f_i(v_r)]}=\frac{1}{2}>
%</div align=center>
%
%In conclusion, it holds that
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\max_{i\in\{1,\cdots,T\}}\mathop{\mathbb{E}}_{S\sim\mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]\geq%201/4>
%</div align=center>
%
%This means that for every algorithm, there exists <img src=http://latex.codecogs.com/gif.latex?f,\mathcal{D}>, such that <img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(f)%3D0> and <img src=http://latex.codecogs.com/gif.latex?\mathop{\mathbb{E}}\limits_{S\sim\mathcal{D}^m}[L_\mathcal{D}(A(S))]\geq%201/4>.
%
%(*following is part of UML Ex5.1*) For a random variable <img src=http://latex.codecogs.com/gif.latex?\theta\in[0,1]> such that <img src=http://latex.codecogs.com/gif.latex?\mathbb{E}(\theta)\geq%201/4>, we have
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?p%28%5Ctheta%5Cgeq%5Cfrac%7B1%7D%7B8%7D%29%3D%5Cint_%5Cfrac%7B1%7D%7B8%7D%5E1%20p%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%20%5Cgeq%5Cint_%5Cfrac%7B1%7D%7B8%7D%5E1%20%5Ctheta%20p%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%3D%5Cmathbb%7BE%7D%28%5Ctheta%29-%5Cint_0%5E%5Cfrac%7B1%7D%7B8%7D%5Ctheta%20p%28%5Ctheta%29%5Ctext%7Bd%7D%5Ctheta%20%5C%5C%20%5Cgeq%5Cmathbb%7BE%7D%28%5Ctheta%29-%5Cfrac%7B1%7D%7B8%7D%5Cint_0%5E%5Cfrac%7B1%7D%7B8%7D%20p%28%5Ctheta%29%5Ctext%7Bd%7D%5Ctheta%20%3D%20%5Cfrac%7B1%7D%7B4%7D%20-%20%5Cfrac%7B1%7D%7B8%7D%5Cleft%28%201-%5Cint%5E1_%5Cfrac%7B1%7D%7B8%7Dp%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%20%5Cright%29>
%</div align=center>
%
%which leads to <img src=http://latex.codecogs.com/gif.latex?p(\theta\geq%201/8)\geq%201/7}>.
%
	NFL theorem tells the neccessity of inductive bias. Philosophically, if someone can explain every phenomenon, his explanations are worthless.
	
\subsection{Agnostic PAC}
	\subsubsection{Beyond realizability assumption}
	In practical, the 'true' labelling function may not exist, and the labels may not be fully determined by the features on hand. Then Agnostic PAC learnability is defined as: training on $m\geq m_\mathcal{H}(\epsilon,\delta)$ samples, there exists an algorithm with \textbf{confidence} at least $1-\delta$ to achieve that:
	
	\begin{equation}
	L_\mathcal{D}(h)\leq\min\limits_{h'\in\mathcal{H}}L_\mathcal{D}(h')+\epsilon
	\end{equation}
in which $L_\mathcal{D}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{(x,y)\sim\mathcal{D}}[h(x)\neq y]\overset{def}{=}\mathcal{D}(\{x:h(x)\neq y\})$.

	\subsubsection{Beyond binary classification}

	
	Agnostic PAC learnability remains the same with:
	
	\begin{equation}
	\mathcal{D}(h)=\mathop{\mathbb{E}}\limits_{x\sim\mathcal{D}}[l(h,z)]
	\end{equation}
in which $l(\cdot)$ is 0-1 loss for multiclass classification and square loss for regression. 

	\subsubsection{Sample complexity under Agn-PAC: via uniform convergence}
%
%  **Def: $\epsilon$-representative** A training set S is called $\epsilon$-representative if
%
%  <div align=center>
%  <img src=http://latex.codecogs.com/gif.latex?\forall%20h\in\mathcal{H},|L_S(h)-L_\mathcal{D}(h)|\leq\epsilon>
%  </div align=center>
%
%  **Thm** : ERM rule is suitable for $\epsilon/2$-representative samples.
%
%  *proof*: for every <img src=http://latex.codecogs.com/gif.latex?h\in\mathcal{H}>,
%
%  <div align=center>
%  <img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(h_S)\leq%20L_S(h)+\frac{\epsilon}{2}\leq%20L_S(h)+\frac{\epsilon}{2}\leq%20L_S(h)+\epsilon>
%  </div align=center>
%
%  Hence, <img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(h_S)\leq%20\min_{h\in\mathcal{H}}L_S(h)+\epsilon>
%
%  **Agnostic PAC sample complexity** Assume that the range of the loss function is <img src=http://latex.codecogs.com/gif.latex?[0,1]>, or more general, <img src=http://latex.codecogs.com/gif.latex?[a,b]>, then a finite hypothsis set <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> enjoys the agnostic PAC learnability with sample complexity
%
%  <div align=center>
%  <img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)\leq%20m^{UC}_\mathcal{H}(\epsilon/2,\delta)\leq\left\lceil\frac{2\log(2|\mathcal{H}|/\delta)(b-a)^2}{\epsilon^2}\right\rceil>
%  </div align=center>
%
\section{Error decomposition}
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(h_S)=\epsilon_{\text{app}}+\epsilon_{\text{est}}>
%</div align=center>
%
%in which
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\epsilon_{\text{app}}=\min\limits_{h\in\mathcal{H}}L_\mathcal{D}(h),\text{%20%20%20%20}\epsilon_{\text{est}}=L_\mathcal{D}(h_S)-\epsilon_{\text{app}}>
%</div align=center>
%
%- The Approximation Error
%
%  This term measures how much risk we have because we restrict ourselves to a specific class, namely, how much *inductive bias* we have. The approximation error does not depend on the sample size and is determined by the hypothesis class chosen. Enlarging the hypothesis class can decrease the approximation error.
%
%- The Estimation Error
%
%  This term results because the empirical risk (i.e., training error) is only an estimate of the true risk. The quality of this estimation depends on the training set size (decreases with it) and on the size, or complexity, of the hypothesis class (logarithmically increases with it).
%

\section{Summary}

Now that, we have come to some important conclusions under the PAC learning framework:

\begin{enumerate}
\item No universal learner;
\item Inductive bias is neccessary to avoid overfitting;
\item Sample complexity is function about hypothesis set, confidence level and error, interestingly, it is nothing to do with the dimension of feature space;
\item Inductive bias controls the balance of approximation error and estimation error.
\end{enumerate}

	We have reached the fundamental question in learning theory: \textbf{Over which hypothesis classes, ERM learning will not result in overfitting (or, PAC learnable)?} Currently, we just confirm the PAC learnability for finite classes. In the next chapter, the most important part in learning theory, VC-dimension, will gives a more precise answer.


\section{Excercises and solutions}
%
%**1.5.1 (UML Ex2.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> be a class of binary classifiers over a domain <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>. Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}> be an unknown distribution over <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>, and let <img src=http://latex.codecogs.com/gif.latex?f> be the target hypothesis in <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}>. Fix some <img src=http://latex.codecogs.com/gif.latex?h\in\mathcal{H}>. Show that the expected value of <img src=http://latex.codecogs.com/gif.latex?L_S(h)> over the choice of <img src=http://latex.codecogs.com/gif.latex?S> equals <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h)>, namely,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=L_{\mathcal{D},f}(h)>
%</div align=center>
%
%Solution: according to the definition,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=\sum_S\mathcal{D}^m(S)\frac{|\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}|}{m}>
%<img src=http://latex.codecogs.com/gif.latex?=\sum_S\mathcal{D}\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}=\mathcal{D}(\{x:h(x)\neq%20f(x)\})_>
%</div align=center>
%
%**1.5.2 (UML Ex2.3) Axis Aligned rectangles** An axis aligned rectangle classifier in the plane is a classifier that assigns the value 1 to a point if and only if it is inside a certain rectangle. Formally, given real numbers <img src=http://latex.codecogs.com/gif.latex?a_1\leq%20b_1,a_2\leq%20b_2>, define the classifier <img src=http://latex.codecogs.com/gif.latex?h(a_1,b_1,a_2,b_2)> by
%
%<div align=center>
%<img src=https://latex.codecogs.com/gif.latex?h%28a_1%2Cb_1%2Ca_2%2Cb_2%29%28x_1%2C%20x_2%29%20%3D%20%5Cleft%5C%7B%5Cbegin%7Baligned%7D%20%261%2C%5Ctext%7B%20if%20%7D%20a_1%20%5Cleq%20x_1%20%5Cleq%20b1%20%5Ctext%7B%20and%20%7D%20a_2%20%5Cleq%20x_2%20%5Cleq%20b_2%20%5C%5C%20%260%2C%5Ctext%7B%20otherwise%7D%20%5Cend%7Baligned%7D%5Cright.>
%</div align=center>
%
%The class of all axis aligned rectangles in the plane is defined as
%
%<div align=center>
%<img src=https://latex.codecogs.com/gif.latex?\mathcal{H}^2_{\text{rec}}=\{h(a_1,b_1,a_2,b_2):a_1\leq%20b_1,\text{and%20}a_2\leq%20b_2\}.>
%</div align=center>
%
%Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.
%
%1. Let <img src=https://latex.codecogs.com/gif.latex?A> be the algorithm that returns the smallest rectangle enclosing all positive examples in the training set. Show that <img src=https://latex.codecogs.com/gif.latex?A> is an ERM.
%
%2. Show that if <img src=https://latex.codecogs.com/gif.latex?A> receives a training set of size <img src=https://latex.codecogs.com/gif.latex?\geq%204\frac{\log(4/\delta)}{\epsilon}>, then, with probability of at least <img src=https://latex.codecogs.com/gif.latex?1-\delta> it returns a hypothesis with error of at most <img src=https://latex.codecogs.com/gif.latex?\epsilon>.
%*Hint*: Fix some distribution <img src=https://latex.codecogs.com/gif.latex?\mathcal{D}> over <img src=https://latex.codecogs.com/gif.latex?\mathcal{X}>, let <img src=https://latex.codecogs.com/gif.latex?R^*%3DR(a^*_1,b^*_1,a^*_2,b^*_2)> be the rectangle that generates the labels, and let <img src=https://latex.codecogs.com/gif.latex?f> be the corresponding hypothesis. Let <img src=https://latex.codecogs.com/gif.latex?a_1\geq%20a^∗_1> be a number such that the probability mass (with respect to <img src=https://latex.codecogs.com/gif.latex?\mathcal{D}>) of the rectangle <img src=https://latex.codecogs.com/gif.latex?R_1%3DR(a^*_1,a_1,a^*_2,b^*_2)> is exactly <img src=https://latex.codecogs.com/gif.latex?\epsilon/4>. Similarly, let <img src=https://latex.codecogs.com/gif.latex?b_1,a_2,b_2> be numbers such that the probability masses of the rectangles <img src=https://latex.codecogs.com/gif.latex?R_2%3DR(b_1,b^*_1,a^*_2,b^*_2)>, <img src=https://latex.codecogs.com/gif.latex?R_3%3DR(a^*_1,b^*_1,a^*_2,a_2)>, <img src=https://latex.codecogs.com/gif.latex?R_4%3DR(a^*_1,b^*_1,b_2,b^*_2)> are all <img src=https://latex.codecogs.com/gif.latex?\epsilon/4>. Let <img src=https://latex.codecogs.com/gif.latex?R(S)> be the rectangle returned by <img src=https://latex.codecogs.com/gif.latex?A>. See illustration in Figure 2.2.
%
%<div align=center>
%<img src="https://i.ibb.co/d2mzC3Y/1.png" alt="1" border="0" />
%</div align=center>
%
%  • Show that <img src=https://latex.codecogs.com/gif.latex?R(S)\subseteq%20R^*>
%
%  • Show that if <img src=https://latex.codecogs.com/gif.latex?S> contains (positive) examples in all of the rectangles <img src=https://latex.codecogs.com/gif.latex?R_1,R_2,R_3,R_4>, then the hypothesis returned by <img src=https://latex.codecogs.com/gif.latex?A> has error of at most <img src=https://latex.codecogs.com/gif.latex?\epsilon>.
%
%  • For each <img src=https://latex.codecogs.com/gif.latex?i\in\{1,\cdots,4\}>, upper bound the probability that <img src=https://latex.codecogs.com/gif.latex?S> does not contain an example from <img src=https://latex.codecogs.com/gif.latex?R_i>.
%
%  • Use the union bound to conclude the argument.
%
%3. Repeat the previous question for the class of axis aligned rectangles in <img src=https://latex.codecogs.com/gif.latex?\mathbb{R}^d>.
%4. Show that the runtime of applying the algorithm <img src=https://latex.codecogs.com/gif.latex?A> mentioned earlier is polynomial in <img src=https://latex.codecogs.com/gif.latex?d,%201/\epsilon>, and in <img src=https://latex.codecogs.com/gif.latex?\log(1/\delta)>.
%
%Solution:
%
%1. In realizable setup, since the tightest rectangle enclosing all positive example is returned, all positive and negative instances are correctly classified.
%
%2. 
%
%Solution:
%
%**1.5.3 (UML Ex3.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}> be a discrete domain, and let <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D%3D%5C%7Bh_z%3Az%5Cin%5Cmathcal%7BX%7D%5C%7D%5Ccup%5C%7Bh%5E-%5C%7D>, where for each <img src=http://latex.codecogs.com/gif.latex?z\in\mathcal{X}>, <img src=http://latex.codecogs.com/gif.latex?h_z> is the function defined by <img src=http://latex.codecogs.com/gif.latex?h_z%28x%29%3D1> if <img src=http://latex.codecogs.com/gif.latex?x%3Dz> and <img src=http://latex.codecogs.com/gif.latex?h_z(x)%3D0> if <img src=http://latex.codecogs.com/gif.latex?x\neq%20z>. <img src=http://latex.codecogs.com/gif.latex?h^-> is simply the all-negative hypothesis, namely, <img src=http://latex.codecogs.com/gif.latex?\forall%20x\in\mathcal{X},h^-(x)%3D0>. The realizability assumption here implies that the true hypothesis <img src=http://latex.codecogs.com/gif.latex?f> labels negatively all examples in the domain, perhaps except one.
%
%1. Describe an algorithm that implements the ERM rule for learning <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D> in the realizable setup.
%2. Show that <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D> is PAC learnable. Provide an upper bound on the sample complexity.
%  
%Solution:
%1. Traverse <img src=http://latex.codecogs.com/gif.latex?z\in\mathcal{X}}> then output <img src=http://latex.codecogs.com/gif.latex?h_z> or <img src=http://latex.codecogs.com/gif.latex?h^->
%2. If for any <img src=http://latex.codecogs.com/gif.latex?i\in[1,\cdots,m]>, such that <img src=http://latex.codecogs.com/gif.latex?h_{x_i}> is the true hypothesis, the algorithm can find it in the realizable setup. Otherwise, the algorithm outputs <img src=http://latex.codecogs.com/gif.latex?h^->, which can be either true or false (i.e., the target <img src=http://latex.codecogs.com/gif.latex?z^*> is not in the training set). Note that in the second case, the algorithm only makes a single error when generalize to all cases, and hence <img src=http://latex.codecogs.com/gif.latex?p(z^*)\geq\epsilon> (otherwise, it is meaningless),
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?\mathbb{P}(L_{\mathcal{D},f}(h_S)%3E\epsilon)\leq(1-p(z^*))^m\leq(1-\epsilon)^m\leq\exp(-\epsilon%20m)\leq\delta>
%</div align=center>
%
%which leads to
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)\leq\left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil>
%</div align=center>
%
%**1.5.4 (UML Ex3.3)** Let <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BX%7D%3D%5Cmathbb%7BR%7D%5E2>, <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BY%7D%3D%5C%7B0%2C1%5C%7D>, and let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> be the class of concentric circles in the plane, that is, <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D%3D%5C%7Bh_r%3Ar%5Cin%5Cmathbb%7BR%7D_&plus;%5C%7D>, where <img src=http://latex.codecogs.com/gif.latex?h_r%28x%29%3D%5Cmathbb%7BI%7D_%7B%5B%5CVert%20x%5CVert%5Cleq%20r%5D%7D>. Prove that <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> is PAC learnable (assume realizability), and its sample complexity is bounded by
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)\leq\left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil>
%</div align=center>
%
%Solution:
%
%**1.5.5 (UML Ex3.4)** 
%
%**1.5.6 (UML Ex3.7)** 
%
%**1.5.7 (UML Ex3.9)** 
%
%**1.5.8 (UML Ex5.3)** Prove that if <img src=http://latex.codecogs.com/gif.latex?|\mathcal{X}|\geq%20km> for a positive integer <img src=http://latex.codecogs.com/gif.latex?k\geq%202>, then we can replace the lower bound in the No-Free-Lunch theorem. Namely, for the task of binary classification, 
%there exists a distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}\sim\mathcal{X}\times\{0,1\}> such that:
%
%- There exists a function <img src=http://latex.codecogs.com/gif.latex?f:\mathcal{X}\rightarrow\{0,1\}> with <img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(f)%3D0>.
%
%- <img src=http://latex.codecogs.com/gif.latex?\mathbb{E}_{S\sim\mathcal{D}^m}[L_\mathcal{D}(A(S))]\geq\frac{1}{2}-\frac{1}{2k}>.
%
%Solution: similar in the proof above, <img src=http://latex.codecogs.com/gif.latex?p\geq(k-1)m>, then
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D}_i}(h)%3D\frac{1}{km}\sum_{x\in%20C}\mathbb{I}_{[h(x)\neq%20f_i(x)]}\geq\frac{1}{km}\sum_{r=1}^p\mathbb{I}_{[h(v_r)\neq%20f_i(v_r)]}\geq\frac{k-1}{pk}\sum_{r=1}^p\mathbb{I}_{[h(v_r)\neq%20f_i(v_r)]}>
%</div align=center>
%
%And similarily,
%
%<div align=center>
%<img src=http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bi%3D1%7D%5ET%20L_%7B%5Cmathcal%7BD%7D_i%7D%20%28A%28S_j%5Ei%29%29\geq\frac{k-1}{k}\min_{r\in\{1,\cdots,p\}}\frac{1}{T}\mathbb{I}_{[A(S^i_j)(v_r)\neq%20f_i(v_r)]}>
%</div align=center>
%
%And the final bound is <img src=http://latex.codecogs.com/gif.latex?1/2-1/2k>.
%
%      To be continue...
%      Chapter 2. VC-dimension
%      Chapter 3. Bayesian-PAC
%      Chapter 4. Generalization in Deep Learning$

\end{document}