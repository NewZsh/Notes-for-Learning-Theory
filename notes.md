# Notes of Statistical Machine Learning Theory

*The notes is mainly based on the following book*

- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)  Shai Shalev-Shwartz and Shai Ben-David, 2014.
- [pattern recognition and machine learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Christopher M. Bishop, 2006.
- [Probabilistic Graphical Models: Principles and Techniques](https://mitpress.mit.edu/books/probabilistic-graphical-models) Daphne Koller and Nir Friedman, 2009.
- [Graphical Models, Exponential Families, and Variational Inference](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf) Martin J. Wainwright and Michael I. Jordan, 2008.

## 1. Probably Approximately Correct (PAC)

*Corresponding to Chapter 2-5 in UML.*

*This part mainly answers the quesion: What can we know about the generalization error? How does the hypothesis set (in application, the choice of classifier, regressor or so on) reflect our prior knowledge, or, inductive bias?*

### 1.1 formulation

**The learner's input**:

- Domain Set: instance $x \in \mathcal{X}$.
- Label Set: label $y \in \mathcal{Y}$. Currently, just consider the binary classification task, i.e., $y = \\{0,1\\}$ or $\\{-1, +1\\}$.
- Training data: $S=((x_1, y_1), \cdots, (x_m,y_m))$ is a finite sequence.

  - **A simple data generation model**: We assume that the instances are generated by some probability distribution $\mathcal{D}$, and there is some 'correct' labeling function (currently): $f:\mathcal{X}\rightarrow\mathcal{Y}$.

  **[IMPORTANT]**: The learner is blind to $\mathcal{D}, f$.

      remark: usually called 'training set', but must be 'training sequence', because the same sample may appear more than one time, and some training algorithms is order-sensitive.
  
**The learner's output**: hypothesis (or classifier, regressor) $h: \mathcal{X}\rightarrow\mathcal{Y}$.

**Generalization error**: *a.k.a*, true error/risk.

$$
L_{\mathcal{D},f}(h) \overset{def}{=} \mathop{\mathbb{P}}\limits_{x \sim \mathcal{D}} [h(x) \neq f(x)] \overset{def}{=} \mathcal{D}(\\{ x:h(x) \neq f(x)\\} )
$$

  **[IMPORTANT]**: by the subscript $\mathcal{D}, f$, it means that the error of $h$ is the probability to draw a random instance $x$, according to the distribution $\mathcal{D}$, such that $h(x)\neq f(x)$.

      remark: here we neglect the measurability assumption.

### 1.2 from Empirical Risk Minimization (ERM) to Probably Approximately Correct (PAC)

**ERM may lead to overfitting**:

- Turn to minimize the Empirial risk:
  
$$
L_{\mathcal{D}}(h) \overset{def}{=} \frac{|\\{i\in \\{1,\cdots,m\\}: h(x_i)\neq y_i\\}|}{m}
$$

- Consider a 'lazy' learner $h$, which predict $y=y_i$ if $x=x_i$, and 0 otherwise, has 1/2 probability to fail for unseen instances, i.e., $L_{\mathcal{D}, f}(h)=1/2$, while $L_{\mathcal{D}}(h)=0$.

**ERM with restricted hypothesis set (inductive bias)**: $h_S\in\arg\min \limits_{h\in\mathcal{H}} L_S(h)$

- Realizability assumption: there exists $h^\* \in\mathcal{H}$, such that $L_{\mathcal{D}, f}(h^*)=0$.

      remark: It implies that $L_S(h^*)=0$, and $L_S(h_S)=0$. However, we are interested in $L_{\mathcal{D}, f}(h_S)$.

- The i.i.d. assumption: the training samples are independently and identically distributed.

- PAC learnability: training on $m\geq m_\mathcal{H}(\epsilon,\delta)$ with *confidence* paramter at least $1-\delta$ to achieve *accuracy* at least $1-\epsilon$.

- Finite hypothesis classes are PAC learnable, and the sample complexity is:

$$ m_\mathcal{H}(\epsilon,\delta)=\log(|\mathcal{H}|/\delta)/\epsilon $$
  
*proof*: Let $\mathcal{H}_B$ be the set of 'bad' hypothesis, that is, $\mathcal{H}_B =\\{h\in\mathcal{H}: L(h_S)  \epsilon \\}$.

Let $M$ be the set of 'misleading' samples, that is, $M=\\{ S|_x:\exists h \in \mathcal{H}_B, L_S(h)=0 \\}$. Note that,

$$M=\mathop{\bigcup}\limits_{h\in\mathcal{H}_B} \\{S|_x: L_S(h)=0\\}$$

Since we would like to bound the probability of the event $L_{\mathcal{D},f}(h_S) > \epsilon$,

$$\mathcal{D}^m(\\{S|_x:L_{\mathcal{D},f}(h_S) > \epsilon\\}) \leq \mathcal{D}^m(M) = \mathcal{D}^m(\cup \limits_{h\in\mathcal{H}_B} \\{S|_x: L_S(h)=0\\}) \leq \sum_{h\in\mathcal{H}_B}\mathcal{D}^m(\\{S|_x: L_S(h)=0\\})}$$


**No-Free-Lunch (neccessity of inductive bias)**:


**Agnostic PAC**:

- Beyond realizability assumption
- Beyond binary classification: learning via uniform convergence

### 1.3 Error decomposition

### 1.4 Summary

Now that, we have come to some important conclusions under the PAC learning framework:

*1. No universal learner;*

*2. inductive bias is neccessary to avoid overfitting;*

*3. sample complexity is function about hypothesis set, confidence level and error, interestingly, it is nothing to do with the dimension of feature space;*

*4. inductive bias controls the balance of approximation error and estimation error.*

And we have reached the fundamental question in learning theory: **Over which hypothesis classes, ERM learning will not result in overfitting (or, PAC learnable)?** Currently, we just confirm the PAC learnability for finite classes. In the next chapter, the most important part in learning theory, VC-dimension, will gives a more precise answer.

### 1.6 Excercises and solutions

#### 1.6.1 (UML Ex2.1)
solution

#### 1.6.2 (UML)

## 2. VC-dimension

### 2.1

## 3. Bayesian-PAC

## 4. Generalization in Deep Learning
