# Notes of Statistical Machine Learning Theory

*The notes is mainly based on the following book*

- UML: [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)  Shai Shalev-Shwartz and Shai Ben-David, 2014.
- PRML: [pattern recognition and machine learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Christopher M. Bishop, 2006.
- PGM: [Probabilistic Graphical Models: Principles and Techniques](https://mitpress.mit.edu/books/probabilistic-graphical-models) Daphne Koller and Nir Friedman, 2009.
- GEV: [Graphical Models, Exponential Families, and Variational Inference](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf) Martin J. Wainwright and Michael I. Jordan, 2008.

## 1. Probably Approximately Correct (PAC)

*Corresponding to Chapter 2-5 in UML.*

*This part mainly answers the quesion: What can we know about the generalization error? How does the hypothesis set (in application, the choice of classifier, regressor or so on) reflect our prior knowledge, or, inductive bias?*

### 1.1 formulation

#### 1.1.1 The learner's input, output, and evaluation

**input**:

- Domain Set: instance <img src=http://latex.codecogs.com/gif.latex?x\in\mathcal{X}>.
- Label Set: label <img src=http://latex.codecogs.com/gif.latex?y\in\mathcal{Y}>. Currently, just consider the binary classification task, i.e., <img src=http://latex.codecogs.com/gif.latex?y%3D%5C%7B0%2C1%5C%7D> or <img src=http://latex.codecogs.com/gif.latex?\{-1,+1\}>.
- Training data: <img src=http://latex.codecogs.com/gif.latex?S%3D%28%28x_1%2Cy_1%29%2C%5Ccdots%2C%28x_m%2Cy_m%29%29> is a finite sequence.

  - **A simple data generation model**: We assume that the instances are generated by some probability distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}>, and there is some 'correct' labeling function (currently): <img src=http://latex.codecogs.com/gif.latex?f:\mathcal{X}\rightarrow\mathcal{Y}>.

  **[IMPORTANT]**: The learner is blind to <img src=http://latex.codecogs.com/gif.latex?\mathcal{D},f>.

      remark: usually called 'training set', but must be 'training sequence', because the same sample may 
      appear more than one time, and some training algorithms is order-sensitive. And for simplification, 
      in the following, I do not distinguish the distribution over the training set or over the instances.
  
**output**: hypothesis (or classifier, regressor) <img src=http://latex.codecogs.com/gif.latex?h:\mathcal{X}\rightarrow\mathcal{Y}>.

**Generalization error**: *a.k.a*, true error/risk.

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{x\sim\mathcal{D}}[h(x)\neq%20f(x)]\overset{def}{=}\mathcal{D}(\{x:h(x)\neq%20f(x)\})>
</div align=center>

  **[IMPORTANT]**: by the subscript <img src=http://latex.codecogs.com/gif.latex?\mathcal{D},f>, it means that the error of <img src=http://latex.codecogs.com/gif.latex?h> is the probability to draw a random instance <img src=http://latex.codecogs.com/gif.latex?x>, according to the distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}>, such that <img src=http://latex.codecogs.com/gif.latex?h(x)\neq%20f(x)>.

      remark: here we neglect the measurability assumption.

### 1.2 from Empirical Risk Minimization (ERM) to Probably Approximately Correct (PAC)

#### 1.2.1 ERM may lead to overfitting

- Since the generalzation error is intractable, turn to minimize the Empirial risk:
  
<div align=center>
<img src=http://latex.codecogs.com/gif.latex?L_S(h)\overset{def}{=}\frac{|(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}|}{m}>
</div align=center>

- Consider a 'lazy' learner <img src=http://latex.codecogs.com/gif.latex?h>, which predict <img src=http://latex.codecogs.com/gif.latex?y%3Dy_i%5C%20%5Ctext%7Biff.%7D%5C%20x%3Dx_i>, and 0 otherwise, has 1/2 probability to fail for unseen instances, i.e., <img src=http://latex.codecogs.com/gif.latex?L_%7B%5Cmathcal%7BD%7D%2Cf%7D%28h%29%3D1/2>, while <img src=http://latex.codecogs.com/gif.latex?L_S%28h%29%3D0>.

#### 1.2.2 ERM with restricted hypothesis set (inductive bias)

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?h_S\in\arg\min\limits_{h\in\mathcal{H}}L_S(h)>
</div align=center>

- Realizability assumption: there exists <img src=http://latex.codecogs.com/gif.latex?h^*\in\mathcal{H}>, such that <img src=http://latex.codecogs.com/gif.latex?L_%7B%5Cmathcal%7BD%7D%2Cf%7D%28h%5E*%29%3D0>.

  **[IMPORTANT]**: It implies that <img src=http://latex.codecogs.com/gif.latex?L_S%28h%5E*%29%3D0>, and <img src=http://latex.codecogs.com/gif.latex?L_S%28h_S%29%3D0>. However, we are interested in <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h_S)>.

- The i.i.d. assumption: the training samples are independently and identically distributed.

- PAC learnability: training on <img src=http://latex.codecogs.com/gif.latex?m\geqm_\mathcal{H}(\epsilon,\delta)> with *confidence* paramter at least <img src=http://latex.codecogs.com/gif.latex?1-\delta> to achieve *accuracy* at least <img src=http://latex.codecogs.com/gif.latex?1-\epsilon>.

- Finite hypothesis classes are PAC learnable, and the sample complexity is:

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)=\log(|\mathcal{H}|/\delta)/\epsilon>
</div align=center>
  
*proof*: Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}_B> be the set of 'bad' hypothesis, that is, <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}_B\subset\mathcal{H}>, and <img src=http://latex.codecogs.com/gif.latex?{\forall}h\in\mathcal{H}_B,L_{\mathcal{D},f}(h)%3E\epsilon>

Let <img src=http://latex.codecogs.com/gif.latex?M> be the set of 'misleading' samples, that is, <img src=http://latex.codecogs.com/gif.latex?M%3D%5C%7BS%3A%7B%5Cexists%7Dh%5Cin%5Cmathcal%7BH%7D_B%2CL_S%28h%29%3D0%5C%7D>. Note that,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?M=\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\}$>
</div align=center>

Since we would like to bound the probability of the event <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h_S)%3E\epsilon>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathcal{D}^m(\{S:L_{\mathcal{D},f}(h_S)%3E\epsilon\})\leq\mathcal{D}^m(M)=\mathcal{D}^m(\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\})>
</div align=center>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?=\sum_{h\in\mathcal{H}_B}\prod_{i=1}^m\mathcal{D}(\{x_i:f(x_i)=h(x_i)\})\overset{i.i.d.}{=}\sum_{h\in\mathcal{H}_B}(1-L_{\mathcal{D},f}(h))^m>
</div align=center>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\leq\sum_{h\in\mathcal{H}_B}(1-\epsilon)^m\leq\sum_{h\in\mathcal{H}_B}\exp(-\epsilon%20m)\leq|\mathcal{H}|\exp(-\epsilon%20m)>
</div align=center>

Let <img src=http://latex.codecogs.com/gif.latex?|\mathcal{H}|\exp(-\epsilon%20m)\leq\delta>, we can solve that <img src=http://latex.codecogs.com/gif.latex?m\geq\log(|\mathcal{H}|/\delta)/\epsilon>

#### 1.2.3 No-Free-Lunch (neccessity of inductive bias)

Let <img src=http://latex.codecogs.com/gif.latex?A> be any learning algorithm for the task of binary classification with respect to the 0-1 loss over a domain <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>. Let <img src=http://latex.codecogs.com/gif.latex?m> be any number smaller than <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}/2>, representing a training set size. Then, there exists a distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}> over <img src=http://latex.codecogs.com/gif.latex?{X}\times\{0,1\}> such that:

1. There exists a function <img src=http://latex.codecogs.com/gif.latex?f:\mathcal{X}\rightartow\{0,1\}> with <img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(f)%3D0>.

2. With probability of at least <img src=http://latex.codecogs.com/gif.latex?1/7> over the choice of <img src=http://latex.codecogs.com/gif.latex?S\sim\mathcal{D}^m> we have that
<img src=http://latex.codecogs.com/gif.latex?L_\mathcal{D}(A(S))\geq%201/8>.

*Proof*: Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{C}\subseteq\mathcal{X}> of size <img src=http://latex.codecogs.com/gif.latex?2m>. There are <img src=http://latex.codecogs.com/gif.latex?T%3D2^{2m}> possible functions from <img src=http://latex.codecogs.com/gif.latex?\mathcal{C}> to <img src=http://latex.codecogs.com/gif.latex?\{0,1\}>. Denote these functions by <img src=http://latex.codecogs.com/gif.latex?f_1,\cdots,f_T>. For each such function, let <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}_i> be a distribution over <img src=http://latex.codecogs.com/gif.latex?\mathcal{C}\times\{0,1\}> defined by

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BD%7D_i%28%5C%7B%28x%2Cy%29%5C%7D%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%201/%7C%5Cmathcal%7BC%7D%7C%2C%20%26%5Ctext%7B%20if%20%7D%20y%3Df_i%28x%29%20%5C%5C%200%2C%26%5Ctext%7Botherwise%7D%20%5Cend%7Bmatrix%7D%5Cright.>
</div align=center>

There are <img src=http://latex.codecogs.com/gif.latex?k%3D(2m)^m> possible sequences of <img src=http://latex.codecogs.com/gif.latex?m> examples from <img src=http://latex.codecogs.com/gif.latex?\mathcal{C}>. Denote these sequences by <img src=http://latex.codecogs.com/gif.latex?S_1,\cdots,S_k>. Also, we denote the sequence <img src=http://latex.codecogs.com/gif.latex?\mathca{S}_j> labeled by the function <img src=http://latex.codecogs.com/gif.latex?f_i> as <img src=http://latex.codecogs.com/gif.latex?\mathcal{S}_j^i>. If the distribuction is <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}_i>, then the possible training sets are <img src=http://latex.codecogs.com/gif.latex?S_1^i,\cdots,S_k^i> (with equal probability). Therefore,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathop{\mathbb{E}}\limits_{S\sim\mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]%3D\frac{1}{k}\sum_{j%3D1}^kL_{\mathcal{D}_i}(A(S_j^i))>
</div align=center>

(*following is part of UML Ex5.1*) For a random variable <img src=http://latex.codecogs.com/gif.latex?\theta\in[0,1]> such that <img src=http://latex.codecogs.com/gif.latex?\mathbb{E}(\theta)\geq%201/4>, we have

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?p%28%5Ctheta%5Cgeq%5Cfrac%7B1%7D%7B8%7D%29%3D%5Cint_%5Cfrac%7B1%7D%7B8%7D%5E1%20p%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%20%5Cgeq%5Cint_%5Cfrac%7B1%7D%7B8%7D%5E1%20%5Ctheta%20p%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%3D%5Cmathbb%7BE%7D%28%5Ctheta%29-%5Cint_0%5E%5Cfrac%7B1%7D%7B8%7D%5Ctheta%20p%28%5Ctheta%29%5Ctext%7Bd%7D%5Ctheta%20%5C%5C%20%5Cgeq%5Cmathbb%7BE%7D%28%5Ctheta%29-%5Cfrac%7B1%7D%7B8%7D%5Cint_0%5E%5Cfrac%7B1%7D%7B8%7D%20p%28%5Ctheta%29%5Ctext%7Bd%7D%5Ctheta%20%3D%20%5Cfrac%7B1%7D%7B4%7D%20-%20%5Cfrac%7B1%7D%7B8%7D%5Cleft%28%201-%5Cint%5E1_%5Cfrac%7B1%7D%7B8%7Dp%28%5Ctheta%29%20%5Ctext%7Bd%7D%5Ctheta%20%5Cright%29>
</div align=center>

which leads to <img src=http://latex.codecogs.com/gif.latex?p(\theta\geq%201/8)\geq%201/7}>.

  **[IMPORTANT]**

#### 1.2.4 Agnostic PAC

- Beyond realizability assumption
- Beyond binary classification: learning via uniform convergence

### 1.3 Error decomposition

### 1.4 Summary

Now that, we have come to some important conclusions under the PAC learning framework:

*1. No universal learner;*

*2. inductive bias is neccessary to avoid overfitting;*

*3. sample complexity is function about hypothesis set, confidence level and error, interestingly, it is nothing to do with the dimension of feature space;*

*4. inductive bias controls the balance of approximation error and estimation error.*

And we have reached the fundamental question in learning theory: **Over which hypothesis classes, ERM learning will not result in overfitting (or, PAC learnable)?** Currently, we just confirm the PAC learnability for finite classes. In the next chapter, the most important part in learning theory, VC-dimension, will gives a more precise answer.

### 1.5 Excercises and solutions

**1.5.1 (UML Ex2.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> be a class of binary classifiers over a domain <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>. Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}> be an unknown distribution over <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>, and let <img src=http://latex.codecogs.com/gif.latex?f> be the target hypothesis in <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}>. Fix some <img src=http://latex.codecogs.com/gif.latex?h\in\mathcal{H}>. Show that the expected value of <img src=http://latex.codecogs.com/gif.latex?L_S(h)> over the choice of <img src=http://latex.codecogs.com/gif.latex?S> equals <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h)>, namely,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=L_{\mathcal{D},f}(h)>
</div align=center>

Solution: according to the definition,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=\sum_S\mathcal{D}^m(S)\frac{|\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}|}{m}>
<img src=http://latex.codecogs.com/gif.latex?=\sum_S\mathcal{D}\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}=\mathcal{D}(\{x:h(x)\neq%20f(x)\})_>
</div align=center>

**1.5.2 (UML Ex2.3) Axis Aligned rectangles** An axis aligned rectangle classifier in the plane is a classifier that assigns the value 1 to a point if and only if it is inside a certain rectangle. Formally, given real numbers <img src=http://latex.codecogs.com/gif.latex?a_1\leq%20b_1,a_2\leq%20b_2>, define the classifier <img src=http://latex.codecogs.com/gif.latex?h(a_1,b_1,a_2,b_2)> by

<div align=center>
<img src=https://latex.codecogs.com/gif.latex?h%28a_1%2Cb_1%2Ca_2%2Cb_2%29%28x_1%2C%20x_2%29%20%3D%20%5Cleft%5C%7B%5Cbegin%7Baligned%7D%20%261%2C%5Ctext%7B%20if%20%7D%20a_1%20%5Cleq%20x_1%20%5Cleq%20b1%20%5Ctext%7B%20and%20%7D%20a_2%20%5Cleq%20x_2%20%5Cleq%20b_2%20%5C%5C%20%260%2C%5Ctext%7B%20otherwise%7D%20%5Cend%7Baligned%7D%5Cright.>
</div align=center>

The class of all axis aligned rectangles in the plane is defined as

<div align=center>
<img src=https://latex.codecogs.com/gif.latex?\mathcal{H}^2_{\text{rec}}=\{h(a_1,b_1,a_2,b_2):a_1\leq%20b_1,\text{and%20}a_2\leq%20b_2\}.>
</div align=center>

Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

1. Let <img src=https://latex.codecogs.com/gif.latex?A> be the algorithm that returns the smallest rectangle enclosing all positive examples in the training set. Show that <img src=https://latex.codecogs.com/gif.latex?A> is an ERM.

2. Show that if <img src=https://latex.codecogs.com/gif.latex?A> receives a training set of size <img src=https://latex.codecogs.com/gif.latex?\geq%204\frac{\log(4/\delta)}{\epsilon}>, then, with probability of at least <img src=https://latex.codecogs.com/gif.latex?1-\delta> it returns a hypothesis with error of at most <img src=https://latex.codecogs.com/gif.latex?\epsilon>.
*Hint*: Fix some distribution <img src=https://latex.codecogs.com/gif.latex?\mathcal{D}> over <img src=https://latex.codecogs.com/gif.latex?\mathcal{X}>, let <img src=https://latex.codecogs.com/gif.latex?R^*%3DR(a^*_1,b^*_1,a^*_2,b^*_2)> be the rectangle that generates the labels, and let <img src=https://latex.codecogs.com/gif.latex?f> be the corresponding hypothesis. Let <img src=https://latex.codecogs.com/gif.latex?a_1\geq%20a^∗_1> be a number such that the probability mass (with respect to <img src=https://latex.codecogs.com/gif.latex?\mathcal{D}>) of the rectangle <img src=https://latex.codecogs.com/gif.latex?R_1%3DR(a^*_1,a_1,a^*_2,b^*_2)> is exactly <img src=https://latex.codecogs.com/gif.latex?\epsilon/4>. Similarly, let <img src=https://latex.codecogs.com/gif.latex?b_1,a_2,b_2> be numbers such that the probability masses of the rectangles <img src=https://latex.codecogs.com/gif.latex?R_2%3DR(b_1,b^*_1,a^*_2,b^*_2)>, <img src=https://latex.codecogs.com/gif.latex?R_3%3DR(a^*_1,b^*_1,a^*_2,a_2)>, <img src=https://latex.codecogs.com/gif.latex?R_4%3DR(a^*_1,b^*_1,b_2,b^*_2)> are all <img src=https://latex.codecogs.com/gif.latex?\epsilon/4>. Let <img src=https://latex.codecogs.com/gif.latex?R(S)> be the rectangle returned by <img src=https://latex.codecogs.com/gif.latex?A>. See illustration in Figure 2.2.

<div align=center>
<img src="https://i.ibb.co/d2mzC3Y/1.png" alt="1" border="0" />
</div align=center>

  • Show that <img src=https://latex.codecogs.com/gif.latex?R(S)\subseteq%20R^*>

  • Show that if <img src=https://latex.codecogs.com/gif.latex?S> contains (positive) examples in all of the rectangles <img src=https://latex.codecogs.com/gif.latex?R_1,R_2,R_3,R_4>, then the hypothesis returned by <img src=https://latex.codecogs.com/gif.latex?A> has error of at most <img src=https://latex.codecogs.com/gif.latex?\epsilon>.

  • For each <img src=https://latex.codecogs.com/gif.latex?i\in\{1,\cdots,4\}>, upper bound the probability that <img src=https://latex.codecogs.com/gif.latex?S> does not contain an example from <img src=https://latex.codecogs.com/gif.latex?R_i>.

  • Use the union bound to conclude the argument.

3. Repeat the previous question for the class of axis aligned rectangles in <img src=https://latex.codecogs.com/gif.latex?\mathbb{R}^d>.
4. Show that the runtime of applying the algorithm <img src=https://latex.codecogs.com/gif.latex?A> mentioned earlier is polynomial in <img src=https://latex.codecogs.com/gif.latex?d,%201/\epsilon>, and in <img src=https://latex.codecogs.com/gif.latex?\log(1/\delta)>.

Solution:

1. In realizable setup, since the tightest rectangle enclosing all positive example is returned, all positive and negative instances are correctly classified.

2. 

**1.5.3 (UML Ex3.1) Monotonicity of Sample Complexity** 1

Solution:

**1.5.4 (UML Ex3.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}> be a discrete domain, and let <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D%3D%5C%7Bh_z%3Az%5Cin%5Cmathcal%7BX%7D%5C%7D%5Ccup%5C%7Bh%5E-%5C%7D>, where for each <img src=http://latex.codecogs.com/gif.latex?z\in\mathcal{X}>, <img src=http://latex.codecogs.com/gif.latex?h_z> is the function defined by <img src=http://latex.codecogs.com/gif.latex?h_z%28x%29%3D1> if <img src=http://latex.codecogs.com/gif.latex?x%3Dz> and <img src=http://latex.codecogs.com/gif.latex?h_z(x)%3D0> if <img src=http://latex.codecogs.com/gif.latex?x\neq%20z>. <img src=http://latex.codecogs.com/gif.latex?h^-> is simply the all-negative hypothesis, namely, <img src=http://latex.codecogs.com/gif.latex?\forall%20x\in\mathcal{X},h^-(x)%3D0>. The realizability assumption here implies that the true hypothesis <img src=http://latex.codecogs.com/gif.latex?f> labels negatively all examples in the domain, perhaps except one.

1. Describe an algorithm that implements the ERM rule for learning <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D> in the realizable setup.
2. Show that <img src=http://latex.codecogs.com/gif.latex?%5Cmathcal%7BH%7D_%7B%5Ctext%7BSingleton%7D%7D> is PAC learnable. Provide an upper bound on the sample complexity.
  
Solution:
1. Traverse <img src=http://latex.codecogs.com/gif.latex?z\in\mathcal{X}}> then output <img src=http://latex.codecogs.com/gif.latex?h_z> or <img src=http://latex.codecogs.com/gif.latex?h^->
2. If for any <img src=http://latex.codecogs.com/gif.latex?i\in[1,\cdots,m]>, such that <img src=http://latex.codecogs.com/gif.latex?h_{x_i}> is the true hypothesis, the algorithm can find it in the realizable setup. Otherwise, the algorithm outputs <img src=http://latex.codecogs.com/gif.latex?h^->, which can be either true or false (i.e., the target <img src=http://latex.codecogs.com/gif.latex?z^*> is not in the training set). Note that in the second case, the algorithm only makes a single error when generalize to all cases, and hence <img src=http://latex.codecogs.com/gif.latex?p(z^*)\geq\epsilon> (otherwise, it is meaningless),

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathbb{P}(L_{\mathcal{D},f}(h_S)%3E\epsilon)\leq(1-p(z^*))^m\leq(1-\epsilon)^m\leq\exp(-\epsilon%20m)\leq\delta>
</div align=center>

which leads to 

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)\leq\lceil\frac{\log(1/\delta)}{\epsilon}\rceil>
</div align=center>

## 2. VC-dimension

### 2.1

## 3. Bayesian-PAC

## 4. Generalization in Deep Learning
