# Notes of Statistical Machine Learning Theory

*The notes is mainly based on the following book*

- UML: [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)  Shai Shalev-Shwartz and Shai Ben-David, 2014.
- PRML: [pattern recognition and machine learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Christopher M. Bishop, 2006.
- PGM: [Probabilistic Graphical Models: Principles and Techniques](https://mitpress.mit.edu/books/probabilistic-graphical-models) Daphne Koller and Nir Friedman, 2009.
- GEV: [Graphical Models, Exponential Families, and Variational Inference](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf) Martin J. Wainwright and Michael I. Jordan, 2008.

## 1. Probably Approximately Correct (PAC)

*Corresponding to Chapter 2-5 in UML.*

*This part mainly answers the quesion: What can we know about the generalization error? How does the hypothesis set (in application, the choice of classifier, regressor or so on) reflect our prior knowledge, or, inductive bias?*

### 1.1 formulation

**The learner's input**:

- Domain Set: instance <img src=http://latex.codecogs.com/gif.latex?x\in\mathcal{X}>.
- Label Set: label <img src=http://latex.codecogs.com/gif.latex?y\in\mathcal{Y}>. Currently, just consider the binary classification task, i.e., <img src=http://latex.codecogs.com/gif.latex?y%3D%5C%7B0%2C1%5C%7D> or <img src=http://latex.codecogs.com/gif.latex?\{-1,+1\}>.
- Training data: <img src=http://latex.codecogs.com/gif.latex?S%3D%28%28x_1%2Cy_1%29%2C%5Ccdots%2C%28x_m%2Cy_m%29%29> is a finite sequence.

  - **A simple data generation model**: We assume that the instances are generated by some probability distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}>, and there is some 'correct' labeling function (currently): <img src=http://latex.codecogs.com/gif.latex?f:\mathcal{X}\rightarrow\mathcal{Y}>.

  **[IMPORTANT]**: The learner is blind to <img src=http://latex.codecogs.com/gif.latex?\mathcal{D},f>.

      remark: usually called 'training set', but must be 'training sequence', because the same sample may 
      appear more than one time, and some training algorithms is order-sensitive. And for simplification, 
      in the following, I do not distinguish the distribution over the training set or over the instances.
  
**The learner's output**: hypothesis (or classifier, regressor) <img src=http://latex.codecogs.com/gif.latex?h:\mathcal{X}\rightarrow\mathcal{Y}>.

**Generalization error**: *a.k.a*, true error/risk.

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h)\overset{def}{=}\mathop{\mathbb{P}}\limits_{x\sim\mathcal{D}}[h(x)\neq%20f(x)]\overset{def}{=}\mathcal{D}(\{x:h(x)\neq%20f(x)\})>
</div align=center>

  **[IMPORTANT]**: by the subscript <img src=http://latex.codecogs.com/gif.latex?\mathcal{D},f>, it means that the error of <img src=http://latex.codecogs.com/gif.latex?h> is the probability to draw a random instance <img src=http://latex.codecogs.com/gif.latex?x>, according to the distribution <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}>, such that <img src=http://latex.codecogs.com/gif.latex?h(x)\neq%20f(x)>.

      remark: here we neglect the measurability assumption.

### 1.2 from Empirical Risk Minimization (ERM) to Probably Approximately Correct (PAC)

**ERM may lead to overfitting**:

- Turn to minimize the Empirial risk:
  
<div align=center>
<img src=http://latex.codecogs.com/gif.latex?L_S(h)\overset{def}{=}\frac{|(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}|}{m}>
</div align=center>

- Consider a 'lazy' learner <img src=http://latex.codecogs.com/gif.latex?h>, which predict <img src=http://latex.codecogs.com/gif.latex?y%3Dy_i%5C%20%5Ctext%7Biff.%7D%5C%20x%3Dx_i>, and 0 otherwise, has 1/2 probability to fail for unseen instances, i.e., <img src=http://latex.codecogs.com/gif.latex?L_%7B%5Cmathcal%7BD%7D%2Cf%7D%28h%29%3D1/2>, while <img src=http://latex.codecogs.com/gif.latex?L_S%28h%29%3D0>.

**ERM with restricted hypothesis set (inductive bias)**: <img src=http://latex.codecogs.com/gif.latex?h_S\in\arg\min\limits_{h\in\mathcal{H}}L_S(h)>

- Realizability assumption: there exists <img src=http://latex.codecogs.com/gif.latex?h^*\in\mathcal{H}>, such that <img src=http://latex.codecogs.com/gif.latex?L_%7B%5Cmathcal%7BD%7D%2Cf%7D%28h%5E*%29%3D0>.

  **[IMPORTANT]**: It implies that <img src=http://latex.codecogs.com/gif.latex?L_S%28h%5E*%29%3D0>, and <img src=http://latex.codecogs.com/gif.latex?L_S%28h_S%29%3D0>. However, we are interested in <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h_S)>.

- The i.i.d. assumption: the training samples are independently and identically distributed.

- PAC learnability: training on <img src=http://latex.codecogs.com/gif.latex?m\geqm_\mathcal{H}(\epsilon,\delta)> with *confidence* paramter at least <img src=http://latex.codecogs.com/gif.latex?1-\delta> to achieve *accuracy* at least <img src=http://latex.codecogs.com/gif.latex?1-\epsilon>.

- Finite hypothesis classes are PAC learnable, and the sample complexity is:

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?m_\mathcal{H}(\epsilon,\delta)=\log(|\mathcal{H}|/\delta)/\epsilon>
</div align=center>
  
*proof*: Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}_B> be the set of 'bad' hypothesis, that is, <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}_B\subset\mathcal{H}>, and <img src=http://latex.codecogs.com/gif.latex?{\forall}h\in\mathcal{H}_B,L_{\mathcal{D},f}(h)%3E\epsilon>

Let <img src=http://latex.codecogs.com/gif.latex?M> be the set of 'misleading' samples, that is, <img src=http://latex.codecogs.com/gif.latex?M%3D%5C%7BS%3A%7B%5Cexists%7Dh%5Cin%5Cmathcal%7BH%7D_B%2CL_S%28h%29%3D0%5C%7D>. Note that,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?M=\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\}$>
</div align=center>

Since we would like to bound the probability of the event <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h_S)%3E\epsilon>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathcal{D}^m(\{S:L_{\mathcal{D},f}(h_S)%3E\epsilon\})\leq\mathcal{D}^m(M)=\mathcal{D}^m(\bigcup\limits_{h\in\mathcal{H}_B}\{S:L_S(h)=0\})>
</div align=center>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?=\sum_{h\in\mathcal{H}_B}\prod_{i=1}^m\mathcal{D}(\{x_i:f(x_i)=h(x_i)\})\overset{i.i.d.}{=}\sum_{h\in\mathcal{H}_B}(1-L_{\mathcal{D},f}(h))^m>
</div align=center>

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\leq\sum_{h\in\mathcal{H}_B}(1-\epsilon)^m\leq\sum_{h\in\mathcal{H}_B}\exp(-\epsilon%20m)\leq|\mathcal{H}|\exp(-\epsilon%20m)>
</div align=center>

Let <img src=http://latex.codecogs.com/gif.latex?|\mathcal{H}|\exp(-\epsilon%20m)\leq\delta>, we can solve that <img src=http://latex.codecogs.com/gif.latex?m\geq\log(|\mathcal{H}|/\delta)/\epsilon>

**No-Free-Lunch (neccessity of inductive bias)**:


**Agnostic PAC**:

- Beyond realizability assumption
- Beyond binary classification: learning via uniform convergence

### 1.3 Error decomposition

### 1.4 Summary

Now that, we have come to some important conclusions under the PAC learning framework:

*1. No universal learner;*

*2. inductive bias is neccessary to avoid overfitting;*

*3. sample complexity is function about hypothesis set, confidence level and error, interestingly, it is nothing to do with the dimension of feature space;*

*4. inductive bias controls the balance of approximation error and estimation error.*

And we have reached the fundamental question in learning theory: **Over which hypothesis classes, ERM learning will not result in overfitting (or, PAC learnable)?** Currently, we just confirm the PAC learnability for finite classes. In the next chapter, the most important part in learning theory, VC-dimension, will gives a more precise answer.

### 1.5 Excercises and solutions

**1.5.1 (UML Ex2.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}> be a class of binary classifiers over a domain <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>. Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{D}> be an unknown distribution over <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}>, and let <img src=http://latex.codecogs.com/gif.latex?f> be the target hypothesis in <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}>. Fix some <img src=http://latex.codecogs.com/gif.latex?h\in\mathcal{H}>. Show that the expected value of <img src=http://latex.codecogs.com/gif.latex?L_S(h)> over the choice of <img src=http://latex.codecogs.com/gif.latex?S> equals <img src=http://latex.codecogs.com/gif.latex?L_{\mathcal{D},f}(h)>, namely,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=L_{\mathcal{D},f}(h)>
</div align=center>

Solution: according to the definition,

<div align=center>
<img src=http://latex.codecogs.com/gif.latex?\mathop\mathbb{E}\limits_{S\sim\mathcal{D}^m}[L_S(h)]=\sum_S\mathcal{D}^m(S)\frac{|\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}|}{m}>
<img src=http://latex.codecogs.com/gif.latex?=\sum_S\mathcal{D}\{(x_i,y_i)\in%20S:h(x_i)\neq%20y_i\}=\mathcal{D}(\{x:h(x)\neq%20f(x)\})_>
</div align=center>

**1.5.2 (UML Ex2.3) Axis Aligned rectangles**

Solution:

**1.5.3 (UML Ex3.1) Monotonicity of Sample Complexity**

Solution:

**1.5.4 (UML Ex3.2)** Let <img src=http://latex.codecogs.com/gif.latex?\mathcal{X}> be a discrete domain, and let <img src=http://latex.codecogs.com/gif.latex?\mathcal{H}=\{h_z:z\in\mathcal{X}\}\cup\{h^-\}>, where for each <img src=http://latex.codecogs.com/gif.latex?z\in\mathcal{X}>, <img src=http://latex.codecogs.com/gif.latex?h_z> is the function defined by <img src=http://latex.codecogs.com/gif.latex?h_z(x)=1> if <img src=http://latex.codecogs.com/gif.latex?x=z> and <img src=http://latex.codecogs.com/gif.latex?h_z(x)=0> if <img src=http://latex.codecogs.com/gif.latex?x\new%20z>. <img src=http://latex.codecogs.com/gif.latex?h^-> is simply the all-negative hypothesis, namely, <img src=http://latex.codecogs.com/gif.latex?\forall%20x\in\mathcal{X},h^-(x)=0>. The realizability assumption here implies that the true hypothesis <img src=http://latex.codecogs.com/gif.latex?f> labels negatively all examples in the domain, perhaps except one.

1. Describe
2. Show that 
## 2. VC-dimension

### 2.1

## 3. Bayesian-PAC

## 4. Generalization in Deep Learning
